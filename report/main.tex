\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Exploring Parameter-Efficient Adaptation and Retrieval for Time Series Foundation Models}

\author{\IEEEauthorblockN{Emanuele Romito}
\IEEEauthorblockA{
\textit{Politecnico di Torino}\\
Turin, Italy \\
s339170@studenti.polito.it}
}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

Time series forecasting has evolved from classical statistical methods such as ARIMA \cite{box2015time} to deep learning approaches. The introduction of Transformer-based architectures marked a significant paradigm shift, with models like Informer \cite{zhou2021informer}, Autoformer \cite{wu2021autoformer}, and PatchTST \cite{nie2023patchtst} demonstrating the efficacy of attention mechanisms for long-sequence modeling. This progression has culminated in the emergence of general-purpose Time Series Foundation Models (TSFMs). Unlike specific models trained from scratch, TSFMs leverage large-scale pre-training on diverse datasets to learn universal temporal patterns, enabling zero-shot transfer capabilities across unseen domains.

Among these emerging foundation models, Chronos-2 \cite{ansari2025chronos2univariateuniversalforecasting} represents a distinct paradigm, adopting a language-modeling approach by quantizing time series values into discrete tokens. A key innovation is its "Group Attention" mechanism, which enables the model to process multiple multivariate series simultaneously, thereby capturing cross-series dependencies. While Chronos-2 exhibits impressive zero-shot performance, the transition to foundation models introduces new challenges in deployment and adaptation.

While TSFMs face various hurdles in real-world application, this research focuses on two specific opportunities for expansion. First, to address the issue of non-stationarity where the standard "pretrain-then-inference" assumption fails, we incorporate PETSA \cite{medeiros2025accurateparameterefficienttesttimeadaptation} for parameter-efficient test-time adaptation. Second, to overcome the "closed world" limitation of finite context windows, we explore Retri-Chronos \cite{han2025retrievalaugmentedtimeseries, ning2025tsragretrievalaugmentedgenerationbased}, utilizing retrieval-augmented generation to access external historical patterns.

\section{Problem Statement}

We define the time series forecasting problem under two distinct settings: Parameter-Efficient Test-Time Adaptation (PETSA) and Retrieval-Augmented Generation (RAG). In both cases, the goal is to predict future values of a target time series.

\subsection{Evaluation Setting 1: Retrieval-Augmented Generation}
\textbf{Task:} The objective of this task is to enhance the forecasting accuracy of a foundation model by retrieving relevant historical contexts from an external database when the local context is insufficient (e.g., in "closed world" scenarios).

\textbf{Input:}
\begin{itemize}
    \item A query time series (current context) $\mathbf{x}_{1:T} = [x_1, \dots, x_T]$.
    \item An external database $\mathcal{D} = \{\mathbf{z}^{(i)}\}_{i=1}^{N}$ containing $N$ historical time series.
\end{itemize}

\textbf{Output:}
\begin{itemize}
    \item A set of $k$ retrieved relevant series $\mathcal{R} = \{\mathbf{z}^{(j)} \mid \mathbf{z}^{(j)} \approx \mathbf{x}_{1:T}\} \subset \mathcal{D}$.
    \item The forecast $\mathbf{x}_{T+1:T+H}$, generated by the model conditioning on both the local context $\mathbf{x}_{1:T}$ and the retrieved context $\mathcal{R}$.
\end{itemize}

\subsection{Evaluation Setting 2: Test-Time Adaptation}
\textbf{Task:} The objective of this task is to adapt a pre-trained TSFM to a specific target time series during inference to handle non-stationarity or domain shifts, modifying only a small set of parameters.

\textbf{Input:}
\begin{itemize}
    \item A target time series context $\mathbf{x}_{1:T} = [x_1, \dots, x_T]$.
    \item A pre-trained foundation model parameterized by $\theta$.
\end{itemize}

\textbf{Output:}
\begin{itemize}
    \item An adapted parameter set $\theta' = \theta + \Delta\theta$, where $\Delta\theta$ represents the update to a small subset of parameters (e.g., LoRA adapters) obtained by minimizing the forecasting error on $\mathbf{x}_{1:T}$.
    \item The forecast $\mathbf{x}_{T+1:T+H}$ generated using the adapted model $\theta'$.
\end{itemize}

\section{Methodology}

To facilitate the exploration of the proposed extensions (PETSA and Retri-Chronos), we first undertook the replication of the Chronos-2 training pipeline. This foundational step was necessary to establish a robust baseline and ensuring that our modifications were applied to a verified codebase, consistent with the reference architecture.

\subsection{Pipeline Overview}
We implemented a modular training framework designed to replicate the Chronos-2 setup. The system consists of the following key components:

\begin{enumerate}
    \item \textbf{Synthetic Data Generation:} Generation of diverse time series using KernelSynth (Gaussian Processes with mixed kernels) to capture universal temporal dynamics.
    \item \textbf{Dataset Building:} Construction of a \texttt{StreamingChronosDataset} that dynamically samples from synthetic and real-world sources.
    \item \textbf{Model Definition:} Implementation of the Chronos-2 architecture based on the T5 encoder backbone, augmented with Group Attention.
    \item \textbf{Training:} Execution of the optimization loop minimizing Weighted Quantile Loss (WQL) with checkpointing and metric logging.
    \item \textbf{Benchmarking:} Validation of the baseline model using a standardized suite before extending the pipeline with PETSA and Retri-Chronos modules.
\end{enumerate}

\section*{Appendix: Model Configuration}
\begin{table}[htbp]
\caption{Chronos-2 (Base) Model Configuration}
\begin{center}
\begin{tabular}{|l|c|}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Parameters & 120M \\
Encoder & T5 \\
Layers & 12 \\
Attention Heads & 12 \\
Hidden Dimension ($d_{model}$) & 768 \\
Feed-Forward Dimension ($d_{ff}$) & 3072 \\
Context Length & 8192 tokens \\
Prediction Horizon & 1024 tokens \\
Patch Size & 16 \\
Dropout & 0.1 \\
\hline
\end{tabular}
\label{tab:model_config}
\end{center}
\end{table}

\section*{Appendix: Training Datasets}
\label{section:dataset-appendix}
The following real-world datasets were used in the training mixture:
\begin{itemize}
    \item \textbf{Monash Repository:} Australian Electricity, Electricity Hourly/Weekly, KDD Cup 2018, Temperature Rain, London Smart Meters.
    \item \textbf{M4 Competition:} Daily, Hourly, Monthly, Weekly.
    \item \textbf{Transportation:} Mexico City Bikes, Taxi (1h, 30min), Uber TLC (Daily, Hourly).
    \item \textbf{Energy & Weather:} Solar (1h), USHCN Daily, Weatherbench (Daily, Hourly, Weekly), Wind Farms (Daily, Hourly).
    \item \textbf{Other:} Wiki Daily 100k.
    \item \textbf{GiftEval (Zero-Shot):} Alibaba Cluster Trace, Azure VM Traces, Borg Cluster Data, Q-Traffic, Buildings 900k.
\end{itemize}

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
