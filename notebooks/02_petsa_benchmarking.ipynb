{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Chronos-2-PETSA Benchmarking\n",
                "\n",
                "This notebook evaluates the Chronos-2-PETSA (Parameter-Efficient Test-Time Adaptation) model on zero-shot datasets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "56239aa4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone Repository\n",
                "!git clone https://github.com/emanueleromito/voyagers-forecasting.git\n",
                "%cd voyagers-forecasting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -e .[dev]\n",
                "!pip install gluonts transformers accelerate typer typer-config rich wandb datasets\n",
                "!pip install --upgrade sympy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import torch\n",
                "import wandb\n",
                "import pandas as pd\n",
                "from huggingface_hub import HfApi\n",
                "from google.colab import userdata\n",
                "\n",
                "sys.path.append(os.path.abspath(\"src\"))\n",
                "\n",
                "from scripts.evaluation.evaluate import eval_pipeline_and_save_results\n",
                "from chronos2.pipeline import Chronos2Pipeline\n",
                "from chronos2.extensions.petsa.petsa import ChronosPETSAWrapper, ChronosPETSAPipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Secrets ---\n",
                "try:\n",
                "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
                "    WANDB_KEY = userdata.get('wandb')\n",
                "    wandb.login(key=WANDB_KEY)\n",
                "except Exception as e:\n",
                "    print(f\"Could not load secrets: {e}\")\n",
                "    HF_TOKEN = None\n",
                "\n",
                "# --- Configuration ---\n",
                "RUN_NAME = \"chronos2-petsa-benchmarking\"\n",
                "MODEL_ID = \"voyagersnlppolito/chronos2-baseline\" \n",
                "BATCH_SIZE = 1 # Must be 1 for instance-level adaptation\n",
                "PETSA_RANK = 8\n",
                "PETSA_ALPHA = 16.0\n",
                "PETSA_STEPS = 5\n",
                "PETSA_LR = 1e-3\n",
                "SPARSE_RATIO = 0.8"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Pretrained Model\n",
                "print(f\"Loading model: {MODEL_ID}\")\n",
                "pipeline = Chronos2Pipeline.from_pretrained(MODEL_ID, device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\", torch_dtype=torch.bfloat16)\n",
                "model = pipeline.model\n",
                "\n",
                "# Wrap with PETSA\n",
                "print(\"Wrapping model with PETSA...\")\n",
                "petsa_wrapper = ChronosPETSAWrapper(model, lora_rank=PETSA_RANK, lora_alpha=PETSA_ALPHA)\n",
                "\n",
                "# Wrap with PETSA Pipeline\n",
                "petsa_pipeline = ChronosPETSAPipeline(petsa_wrapper)\n",
                "print(\"PETSA Pipeline ready.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run Zero-Shot Evaluation\n",
                "print(\"Starting Zero-Shot Evaluation with PETSA...\")\n",
                "ZERO_SHOT_RESULTS_PATH = \"evaluation_results_petsa_zero_shot.csv\"\n",
                "ZERO_SHOT_CONFIG_PATH = \"scripts/evaluation/configs/zero-shot.yaml\"\n",
                "\n",
                "if not os.path.exists(ZERO_SHOT_CONFIG_PATH):\n",
                "    print(f\"Config file not found at {ZERO_SHOT_CONFIG_PATH}. Please check the path.\")\n",
                "else:\n",
                "    # Run evaluation\n",
                "    eval_pipeline_and_save_results(\n",
                "        pipeline=petsa_pipeline,\n",
                "        config_path=ZERO_SHOT_CONFIG_PATH,\n",
                "        metrics_path=ZERO_SHOT_RESULTS_PATH,\n",
                "        model_id=RUN_NAME,\n",
                "        batch_size=BATCH_SIZE,\n",
                "        sparse_ratio=SPARSE_RATIO,\n",
                "    )\n",
                "\n",
                "    # Log to WandB\n",
                "    if wandb.run is not None:\n",
                "        print(\"Logging results to WandB...\")\n",
                "        try:\n",
                "            results_df = pd.read_csv(ZERO_SHOT_RESULTS_PATH)\n",
                "            wandb.log({\"evaluation_results_petsa\": wandb.Table(dataframe=results_df)})\n",
                "            \n",
                "            # Log aggregate metrics\n",
                "            avg_mase = results_df[\"MASE\"].mean()\n",
                "            avg_wql = results_df[\"WQL\"].mean()\n",
                "            wandb.log({\"eval/petsa_avg_mase\": avg_mase, \"eval/petsa_avg_wql\": avg_wql})\n",
                "            print(\"Logged results to WandB.\")\n",
                "        except Exception as e:\n",
                "            print(f\"Failed to log to WandB: {e}\")\n",
                "\n",
                "    # Push to Hub\n",
                "    if HF_TOKEN:\n",
                "        print(\"Uploading results to Hub...\")\n",
                "        api = HfApi()\n",
                "        try:\n",
                "            api.upload_file(\n",
                "                path_or_fileobj=ZERO_SHOT_RESULTS_PATH,\n",
                "                path_in_repo=\"evaluation_results_petsa_zero_shot.csv\",\n",
                "                repo_id=f\"voyagersnlppolito/{RUN_NAME}\",\n",
                "                repo_type=\"model\"\n",
                "            )\n",
                "            print(\"Results uploaded successfully.\")\n",
                "        except Exception as e:\n",
                "            print(f\"Failed to upload results: {e}\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}