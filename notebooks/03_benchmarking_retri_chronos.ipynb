{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Retri-Chronos Benchmarking\n",
                "\n",
                "This notebook evaluates the **Retri-Chronos** (Retrieval-Augmented Forecasting) model on zero-shot datasets.\n",
                "For each dataset, we use the *training split* to build a **TimeSeriesKnowledgeBase**, and then forecast the *test split* using the retrieval-augmented pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import yaml\n",
                "import torch\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import datasets\n",
                "import wandb\n",
                "from pathlib import Path\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# Add src to path\n",
                "sys.path.append(os.path.abspath(\"../src\"))\n",
                "\n",
                "from chronos2.model import Chronos2Model\n",
                "from chronos2.pipeline import Chronos2Pipeline\n",
                "from chronos2.extensions.retri_chronos.retri_chronos import TimeSeriesKnowledgeBase, RetriChronosPipeline\n",
                "\n",
                "# Evaluation utilities\n",
                "from gluonts.dataset.split import split\n",
                "from gluonts.ev.metrics import MASE, MeanWeightedSumQuantileLoss\n",
                "from gluonts.model.evaluation import evaluate_forecasts\n",
                "from scripts.evaluation.evaluate import generate_forecasts, QUANTILES"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Configuration ---\n",
                "RUN_NAME = \"retri-chronos-benchmarking\"\n",
                "MODEL_ID = \"voyagersforecasting/chronos2-baseline\"\n",
                "BATCH_SIZE = 16\n",
                "RETRIEVAL_K = 2 # Number of neighbors to retrieve\n",
                "CONFIG_PATH = \"../scripts/evaluation/configs/zero-shot.yaml\"\n",
                "RESULTS_PATH = \"evaluation_results_retri_chronos.csv\"\n",
                "\n",
                "# Select a subset of datasets for quick testing, or None for all\n",
                "# DATASET_FILTER = [\"monash_traffic\", \"monash_weather\"] \n",
                "DATASET_FILTER = [\"monash_traffic\"] # Start small"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Data Loading Utilities ---\n",
                "# Adapted from scripts/evaluation/evaluate.py to return TRAIN split for KB\n",
                "\n",
                "def to_gluonts_univariate(hf_dataset: datasets.Dataset):\n",
                "    series_fields = [col for col in hf_dataset.features if isinstance(hf_dataset.features[col], datasets.Sequence)]\n",
                "    series_fields.remove(\"timestamp\")\n",
                "    \n",
                "    # Assumes that all time series in the dataset have the same frequency\n",
                "    dataset_freq = pd.DatetimeIndex(hf_dataset[0][\"timestamp\"]).to_period()[0].freqstr\n",
                "\n",
                "    gts_dataset = []\n",
                "    for hf_entry in hf_dataset:\n",
                "        for field in series_fields:\n",
                "            gts_dataset.append(\n",
                "                {\n",
                "                    \"start\": pd.Period(\n",
                "                        hf_entry[\"timestamp\"][0],\n",
                "                        freq=dataset_freq,\n",
                "                    ),\n",
                "                    \"target\": hf_entry[field],\n",
                "                }\n",
                "            )\n",
                "    return gts_dataset\n",
                "\n",
                "def load_split_and_get_train_test(backtest_config: dict):\n",
                "    hf_repo = backtest_config[\"hf_repo\"]\n",
                "    dataset_name = backtest_config[\"name\"]\n",
                "    offset = backtest_config[\"offset\"]\n",
                "    prediction_length = backtest_config[\"prediction_length\"]\n",
                "    num_rolls = backtest_config[\"num_rolls\"]\n",
                "\n",
                "    trust_remote_code = True if hf_repo == \"autogluon/chronos_datasets_extra\" else False\n",
                "\n",
                "    print(f\"Loading {dataset_name} from {hf_repo}...\")\n",
                "    ds = datasets.load_dataset(hf_repo, dataset_name, split=\"train\", trust_remote_code=trust_remote_code)\n",
                "    ds.set_format(\"numpy\")\n",
                "\n",
                "    gts_dataset = to_gluonts_univariate(ds)\n",
                "\n",
                "    # Split dataset: Train (History) vs Test (Future targets)\n",
                "    # The 'split' function returns (training_dataset, test_template)\n",
                "    train_data, test_template = split(gts_dataset, offset=offset)\n",
                "    \n",
                "    # Generate test instances (windows)\n",
                "    test_data = test_template.generate_instances(prediction_length, windows=num_rolls)\n",
                "\n",
                "    return train_data, test_data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Model Loading ---\n",
                "print(f\"Loading base model: {MODEL_ID}\")\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "pipeline = Chronos2Pipeline.from_pretrained(MODEL_ID, device_map=device, torch_dtype=torch.bfloat16)\n",
                "model = pipeline.model\n",
                "\n",
                "# Note: RetriChronosPipeline will be initialized per dataset after building KB"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Benchmarking Loop ---\n",
                "\n",
                "with open(CONFIG_PATH) as fp:\n",
                "    all_configs = yaml.safe_load(fp)\n",
                "\n",
                "if DATASET_FILTER:\n",
                "    configs_to_run = [c for c in all_configs if c[\"name\"] in DATASET_FILTER]\n",
                "else:\n",
                "    configs_to_run = all_configs\n",
                "\n",
                "print(f\"Running benchmark on {len(configs_to_run)} datasets.\")\n",
                "result_rows = []\n",
                "\n",
                "for config in configs_to_run:\n",
                "    dataset_name = config[\"name\"]\n",
                "    prediction_length = config[\"prediction_length\"]\n",
                "    \n",
                "    # 1. Load Data\n",
                "    train_data, test_data = load_split_and_get_train_test(config)\n",
                "    \n",
                "    # 2. Build Knowledge Base\n",
                "    print(f\"Building Knowledge Base for {dataset_name}...\")\n",
                "    kb = TimeSeriesKnowledgeBase(model, dimension=model.config.d_model, index_type=\"FlatL2\")\n",
                "    \n",
                "    # Convert GluonTS train_data (list of dicts) to list of tensors for KB\n",
                "    # Filter out short series if necessary, or let KB handle it\n",
                "    train_tensors = [torch.tensor(entry[\"target\"]) for entry in train_data]\n",
                "    kb.build_index(train_tensors, batch_size=BATCH_SIZE)\n",
                "    \n",
                "    # 3. Initialize Retri-Chronos Pipeline\n",
                "    retri_pipeline = RetriChronosPipeline(model, kb)\n",
                "    \n",
                "    # 4. Generate Forecasts\n",
                "    print(f\"Generating forecasts (k={RETRIEVAL_K})...\")\n",
                "    forecasts = generate_forecasts(\n",
                "        test_data.input,\n",
                "        pipeline=retri_pipeline,\n",
                "        prediction_length=prediction_length,\n",
                "        batch_size=BATCH_SIZE,\n",
                "        k=RETRIEVAL_K # Pass k to predict\n",
                "    )\n",
                "    \n",
                "    # 5. Evaluate\n",
                "    print(f\"Evaluating {dataset_name}...\")\n",
                "    metrics = (\n",
                "        evaluate_forecasts(\n",
                "            forecasts,\n",
                "            test_data=test_data,\n",
                "            metrics=[\n",
                "                MASE(),\n",
                "                MeanWeightedSumQuantileLoss(QUANTILES),\n",
                "            ],\n",
                "            batch_size=5000,\n",
                "        )\n",
                "        .reset_index(drop=True)\n",
                "        .to_dict(orient=\"records\")\n",
                "    )\n",
                "    \n",
                "    row = {\"dataset\": dataset_name, \"model\": f\"RetriChronos(k={RETRIEVAL_K})\", **metrics[0]}\n",
                "    result_rows.append(row)\n",
                "    print(f\"Finished {dataset_name}: MASE={row['MASE[0.5]']:.4f}, WQL={row['mean_weighted_sum_quantile_loss']:.4f}\")\n",
                "\n",
                "\n",
                "# Save Results\n",
                "results_df = (\n",
                "    pd.DataFrame(result_rows)\n",
                "    .rename(\n",
                "        {\"MASE[0.5]\": \"MASE\", \"mean_weighted_sum_quantile_loss\": \"WQL\"},\n",
                "        axis=\"columns\",\n",
                "    )\n",
                "    .sort_values(by=\"dataset\")\n",
                ")\n",
                "results_df.to_csv(RESULTS_PATH, index=False)\n",
                "print(f\"Results saved to {RESULTS_PATH}\")\n",
                "results_df"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
