{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Chronos-2 Training & Benchmarking\n",
                "\n",
                "This notebook establishes a training baseline for Chronos-2 using synthetic data and evaluates on standard benchmarks."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone Repository\n",
                "!git clone https://github.com/emanueleromito/voyagers-forecasting.git\n",
                "%cd voyagers-forecasting\n",
                "\n",
                "# Create checkpoint directory\n",
                "import os\n",
                "CHECKPOINT_DIR = './checkpoints'\n",
                "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -e .[dev]\n",
                "!pip install gluonts transformers accelerate typer typer-config rich wandb datasets gift-eval"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\n",
                "import math\n",
                "import random\n",
                "import sys\n",
                "import os\n",
                "from pathlib import Path\n",
                "from typing import List, Optional, Iterator\n",
                "from functools import partial\n",
                "import itertools\n",
                "from google.colab import userdata\n",
                "\n",
                "# Ensure src is in python path\n",
                "sys.path.append(os.path.abspath(\"src\"))\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "import transformers\n",
                "from transformers import TrainingArguments\n",
                "from gluonts.dataset.common import FileDataset\n",
                "import wandb\n",
                "\n",
                "# Import Chronos-2 components\n",
                "from chronos2.model import Chronos2Model\n",
                "from chronos2.config import Chronos2CoreConfig, Chronos2ForecastingConfig\n",
                "from chronos2.dataset import Chronos2Dataset, DatasetMode\n",
                "from chronos2.trainer import Chronos2Trainer\n",
                "from chronos2.benchmarking import run_benchmark\n",
                "\n",
                "# Setup logging\n",
                "logging.basicConfig(format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=logging.INFO)\n",
                "logger = logging.getLogger(__name__)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.1 Configuration & Hyperparameters\n",
                "\n",
                "Centralized configuration for data generation, model architecture, and training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Reproducibility ---\n",
                "SEED = 42\n",
                "\n",
                "# --- Data Generation (KernelSynth) ---\n",
                "DATA_LENGTH = 1024\n",
                "NUM_SAMPLES = 1000\n",
                "MAX_KERNELS = 5\n",
                "PERIODICITIES = [24, 48, 96, 168, 336, 720, 1440, 8760, 17520]\n",
                "LENGTH_SCALES = [0.1, 1.0, 10.0]\n",
                "DATA_PATH = Path(\"kernelsynth-data-paper.arrow\")\n",
                "\n",
                "# --- Model Configuration (Chronos-2 Tiny ~15M) ---\n",
                "CONTEXT_LENGTH = 512\n",
                "PREDICTION_LENGTH = 64\n",
                "PATCH_SIZE = 8\n",
                "D_MODEL = 256\n",
                "D_KV = 32\n",
                "D_FF = 1024\n",
                "NUM_LAYERS = 4\n",
                "NUM_HEADS = 4\n",
                "DROPOUT_RATE = 0.1\n",
                "VOCAB_SIZE = 2\n",
                "QUANTILES = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
                "\n",
                "# --- Training Configuration ---\n",
                "BATCH_SIZE = 32\n",
                "LEARNING_RATE = 1e-3\n",
                "MAX_STEPS = 20000\n",
                "SAVE_STEPS = 100\n",
                "LOGGING_STEPS = 10\n",
                "WARMUP_RATIO = 0.0\n",
                "RUN_NAME = \"chronos2-tiny-baseline\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.2 Reproducibility Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def set_seed(seed: int = 42):\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    torch.cuda.manual_seed_all(seed)\n",
                "    transformers.set_seed(seed)\n",
                "    \n",
                "    # Ensure deterministic behavior in PyTorch (may impact performance)\n",
                "    torch.backends.cudnn.deterministic = True\n",
                "    torch.backends.cudnn.benchmark = False\n",
                "    \n",
                "    print(f\"Random seed set to {seed}\")\n",
                "\n",
                "set_seed(SEED)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.3 Weights & Biases Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wandb.login(key=userdata.get('wandb'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Generation (Synthetic)\n",
                "\n",
                "We use KernelSynth to generate synthetic time series data, aligned with Chronos-2 paper specifications."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import functools\n",
                "from sklearn.gaussian_process import GaussianProcessRegressor\n",
                "from sklearn.gaussian_process.kernels import (\n",
                "    RBF,\n",
                "    ConstantKernel,\n",
                "    DotProduct,\n",
                "    ExpSineSquared,\n",
                "    Kernel,\n",
                "    RationalQuadratic,\n",
                "    WhiteKernel,\n",
                ")\n",
                "from gluonts.dataset.arrow import ArrowWriter\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# --- KernelSynth Logic (Aligned with Paper) ---\n",
                "\n",
                "KERNEL_BANK = [\n",
                "    *[ExpSineSquared(periodicity=p / DATA_LENGTH) for p in PERIODICITIES],\n",
                "    *[RBF(length_scale=l) for l in LENGTH_SCALES],\n",
                "    DotProduct(sigma_0=0.0),\n",
                "    DotProduct(sigma_0=1.0),\n",
                "    RationalQuadratic(alpha=0.1),\n",
                "    RationalQuadratic(alpha=1.0),\n",
                "    WhiteKernel(noise_level=0.1),\n",
                "    WhiteKernel(noise_level=1.0),\n",
                "    ConstantKernel(),\n",
                "]\n",
                "\n",
                "def random_binary_map(a: Kernel, b: Kernel):\n",
                "    binary_maps = [lambda x, y: x + y, lambda x, y: x * y]\n",
                "    return np.random.choice(binary_maps)(a, b)\n",
                "\n",
                "def sample_from_gp_prior(kernel: Kernel, X: np.ndarray, random_seed: Optional[int] = None):\n",
                "    if X.ndim == 1:\n",
                "        X = X[:, None]\n",
                "    gpr = GaussianProcessRegressor(kernel=kernel)\n",
                "    ts = gpr.sample_y(X, n_samples=1, random_state=random_seed)\n",
                "    return ts\n",
                "\n",
                "def generate_time_series(max_kernels: int = MAX_KERNELS):\n",
                "    while True:\n",
                "        X = np.linspace(0, 1, DATA_LENGTH)\n",
                "        selected_kernels = np.random.choice(\n",
                "            KERNEL_BANK, np.random.randint(1, max_kernels + 1), replace=True\n",
                "        )\n",
                "        kernel = functools.reduce(random_binary_map, selected_kernels)\n",
                "        try:\n",
                "            ts = sample_from_gp_prior(kernel=kernel, X=X)\n",
                "            return {\"start\": np.datetime64(\"2000-01-01 00:00\", \"s\"), \"target\": ts.squeeze()}\n",
                "        except Exception:\n",
                "            continue\n",
                "\n",
                "# Generate Data\n",
                "if not DATA_PATH.exists():\n",
                "    print(\"Generating synthetic data (Paper Config)...\")\n",
                "    generated_dataset = [\n",
                "        generate_time_series(max_kernels=MAX_KERNELS)\n",
                "        for _ in tqdm(range(NUM_SAMPLES))\n",
                "    ]\n",
                "    ArrowWriter(compression=\"lz4\").write_to_file(\n",
                "        generated_dataset,\n",
                "        path=DATA_PATH,\n",
                "    )\n",
                "    print(f\"Data saved to {DATA_PATH}\")\n",
                "else:\n",
                "    print(f\"Data already exists at {DATA_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Dataset Preparation\n",
                "\n",
                "We load the data and prepare it for `Chronos2Dataset`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data from Arrow file\n",
                "raw_dataset = FileDataset(DATA_PATH, freq=\"h\")\n",
                "\n",
                "# Convert to list of dicts\n",
                "data = [{\"target\": entry[\"target\"]} for entry in raw_dataset]\n",
                "\n",
                "# Split into train/validation\n",
                "split_idx = int(len(data) * 0.9)\n",
                "train_data = data[:split_idx]\n",
                "val_data = data[split_idx:]\n",
                "\n",
                "print(f\"Training samples: {len(train_data)}\")\n",
                "print(f\"Validation samples: {len(val_data)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Initialization & Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Chronos-2 Forecasting Config\n",
                "chronos_forecasting_config = Chronos2ForecastingConfig(\n",
                "    context_length=CONTEXT_LENGTH,\n",
                "    output_patch_size=PATCH_SIZE,\n",
                "    input_patch_size=PATCH_SIZE,\n",
                "    input_patch_stride=PATCH_SIZE,\n",
                "    quantiles=QUANTILES,\n",
                "    time_encoding_scale=CONTEXT_LENGTH,\n",
                "    use_reg_token=True,\n",
                ")\n",
                "\n",
                "# Chronos-2 Core Config (Tiny ~15M)\n",
                "model_config = Chronos2CoreConfig(\n",
                "    d_model=D_MODEL,\n",
                "    d_kv=D_KV,\n",
                "    d_ff=D_FF,\n",
                "    num_layers=NUM_LAYERS,\n",
                "    num_heads=NUM_HEADS,\n",
                "    dropout_rate=DROPOUT_RATE,\n",
                "    vocab_size=VOCAB_SIZE,\n",
                ")\n",
                "model_config.chronos_config = chronos_forecasting_config.__dict__\n",
                "\n",
                "# Initialize Model\n",
                "model = Chronos2Model(model_config)\n",
                "print(f\"Model Parameters: {model.num_parameters() / 1e6:.2f}M\")\n",
                "\n",
                "# Prepare Datasets\n",
                "train_ds = Chronos2Dataset(\n",
                "    inputs=train_data,\n",
                "    context_length=CONTEXT_LENGTH,\n",
                "    prediction_length=PREDICTION_LENGTH,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    output_patch_size=PATCH_SIZE,\n",
                "    mode=DatasetMode.TRAIN,\n",
                ")\n",
                "\n",
                "val_ds = Chronos2Dataset(\n",
                "    inputs=val_data,\n",
                "    context_length=CONTEXT_LENGTH,\n",
                "    prediction_length=PREDICTION_LENGTH,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    output_patch_size=PATCH_SIZE,\n",
                "    mode=DatasetMode.VALIDATION,\n",
                ")\n",
                "\n",
                "# Training Arguments\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=CHECKPOINT_DIR,\n",
                "    per_device_train_batch_size=BATCH_SIZE,\n",
                "    per_device_eval_batch_size=BATCH_SIZE,\n",
                "    learning_rate=LEARNING_RATE,\n",
                "    lr_scheduler_type=\"linear\",\n",
                "    warmup_ratio=WARMUP_RATIO,\n",
                "    num_train_epochs=5,\n",
                "    max_steps=MAX_STEPS,\n",
                "    save_steps=SAVE_STEPS,\n",
                "    logging_steps=LOGGING_STEPS,\n",
                "    save_strategy=\"steps\",\n",
                "    fp16=False,\n",
                "    dataloader_num_workers=2,\n",
                "    remove_unused_columns=False,\n",
                "    report_to=\"wandb\",\n",
                "    run_name=RUN_NAME,\n",
                "    seed=SEED,\n",
                "    data_seed=SEED,\n",
                ")\n",
                "\n",
                "# Trainer\n",
                "trainer = Chronos2Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_ds,\n",
                "    eval_dataset=val_ds,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Run Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Starting training...\")\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Running final validation...\")\n",
                "eval_results = trainer.evaluate()\n",
                "print(f\"Final Validation Results: {eval_results}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Benchmarking\n",
                "\n",
                "We evaluate the trained model on standard benchmarks (Monash and GIFT-Eval)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define datasets to benchmark\n",
                "benchmark_datasets = [\n",
                "    # Monash datasets (quick evaluation)\n",
                "    {'name': 'electricity', 'type': 'monash', 'prediction_length': 24, 'max_samples': 50},\n",
                "    {'name': 'traffic', 'type': 'monash', 'prediction_length': 24, 'max_samples': 50},\n",
                "    {'name': 'm4_hourly', 'type': 'monash', 'prediction_length': 48, 'max_samples': 50},\n",
                "    \n",
                "    # GIFT-Eval datasets (comprehensive evaluation)\n",
                "    {'name': 'm4_weekly', 'type': 'gift-eval', 'prediction_length': 13, 'term': 'short'},\n",
                "    {'name': 'm4_monthly', 'type': 'gift-eval', 'prediction_length': 18, 'term': 'short'},\n",
                "    {'name': 'm4_quarterly', 'type': 'gift-eval', 'prediction_length': 8, 'term': 'short'},\n",
                "    {'name': 'm4_yearly', 'type': 'gift-eval', 'prediction_length': 6, 'term': 'short'},\n",
                "]\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"RUNNING BENCHMARKS\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Total datasets: {len(benchmark_datasets)}\")\n",
                "print(f\"Monash: {sum(1 for d in benchmark_datasets if d['type'] == 'monash')}\")\n",
                "print(f\"GIFT-Eval: {sum(1 for d in benchmark_datasets if d['type'] == 'gift-eval')}\")\n",
                "\n",
                "# Run benchmark\n",
                "results_df = run_benchmark(\n",
                "    model=model,\n",
                "    datasets=benchmark_datasets,\n",
                "    batch_size=32,\n",
                ")\n",
                "\n",
                "# Display results\n",
                "if not results_df.empty:\n",
                "    cols = ['dataset', 'type', 'MASE', 'MAE', 'RMSE', 'wQuantileLoss[0.5]', 'wQuantileLoss[0.9]', 'CRPS']\n",
                "    cols = [c for c in cols if c in results_df.columns]\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"BENCHMARK RESULTS\")\n",
                "    print(\"=\"*60)\n",
                "    print(results_df[cols].to_string(index=False))\n",
                "    \n",
                "    # Summary by type\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"SUMMARY BY TYPE\")\n",
                "    print(\"=\"*60)\n",
                "    summary = results_df.groupby('type')[['MASE', 'MAE', 'RMSE']].mean()\n",
                "    print(summary)\n",
                "    \n",
                "    # Log to WandB\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"LOGGING TO WANDB\")\n",
                "    print(\"=\"*60)\n",
                "    \n",
                "    # Log individual dataset results\n",
                "    for _, row in results_df.iterrows():\n",
                "        dataset_name = row['dataset']\n",
                "        dataset_type = row['type']\n",
                "        \n",
                "        # Create metrics dict\n",
                "        metrics = {}\n",
                "        for col in results_df.columns:\n",
                "            if col not in ['dataset', 'type']:\n",
                "                metrics[f\"benchmark/{dataset_type}/{dataset_name}/{col}\"] = row[col]\n",
                "        \n",
                "        wandb.log(metrics)\n",
                "    \n",
                "    # Log summary statistics\n",
                "    for ds_type in summary.index:\n",
                "        for metric in summary.columns:\n",
                "            wandb.log({f\"benchmark_summary/{ds_type}/{metric}\": summary.loc[ds_type, metric]})\n",
                "    \n",
                "    # Create and log a WandB table\n",
                "    wandb.log({\"benchmark_results\": wandb.Table(dataframe=results_df)})\n",
                "    \n",
                "    print(\"✓ Benchmark results logged to WandB\")\n",
                "    \n",
                "    # Save to CSV\n",
                "    results_df.to_csv('benchmark_results.csv', index=False)\n",
                "    print(\"\\n✓ Results saved to benchmark_results.csv\")\n",
                "else:\n",
                "    print(\"No benchmark results generated.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}