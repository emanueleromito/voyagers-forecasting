{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Chronos-2 Training Baseline\n",
                "\n",
                "This notebook establishes a training baseline for Chronos-2 using synthetic data. It is designed to run on Google Colab."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone Repository\n",
                "!git clone https://github.com/emanueleromito/voyagers-forecasting.git\n",
                "%cd voyagers-forecasting\n",
                "\n",
                "# Create checkpoint directory\n",
                "import os\n",
                "CHECKPOINT_DIR = './checkpoints'\n",
                "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -e .[dev]\n",
                "!pip install gluonts transformers accelerate typer typer-config rich wandb"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\n",
                "import math\n",
                "import random\n",
                "import sys\n",
                "import os\n",
                "from pathlib import Path\n",
                "from typing import List, Optional, Iterator\n",
                "from functools import partial\n",
                "import itertools\n",
                "\n",
                "# Ensure src is in python path\n",
                "sys.path.append(os.path.abspath(\"src\"))\n",
                "\n",
                "import numpy as np\n",
                "import torch\n",
                "import transformers\n",
                "from transformers import TrainingArguments\n",
                "from gluonts.dataset.common import FileDataset\n",
                "import wandb\n",
                "\n",
                "# Import Chronos-2 components\n",
                "from chronos2.model import Chronos2Model\n",
                "from chronos2.config import Chronos2CoreConfig, Chronos2ForecastingConfig\n",
                "from chronos2.dataset import Chronos2Dataset, DatasetMode\n",
                "from chronos2.trainer import Chronos2Trainer\n",
                "\n",
                "# Setup logging\n",
                "logging.basicConfig(format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=logging.INFO)\n",
                "logger = logging.getLogger(__name__)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.1 Reproducibility\n",
                "\n",
                "We set random seeds to ensure reproducible results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def set_seed(seed: int = 42):\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    torch.cuda.manual_seed_all(seed)\n",
                "    transformers.set_seed(seed)\n",
                "    \n",
                "    # Ensure deterministic behavior in PyTorch (may impact performance)\n",
                "    torch.backends.cudnn.deterministic = True\n",
                "    torch.backends.cudnn.benchmark = False\n",
                "    \n",
                "    print(f\"Random seed set to {seed}\")\n",
                "\n",
                "SEED = 42\n",
                "set_seed(SEED)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.2 Weights & Biases Setup\n",
                "\n",
                "Login to Weights & Biases to track experiments."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wandb.login()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Generation (Synthetic)\n",
                "\n",
                "We use a simplified version of KernelSynth to generate synthetic time series data on the fly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import functools\n",
                "from sklearn.gaussian_process import GaussianProcessRegressor\n",
                "from sklearn.gaussian_process.kernels import (\n",
                "    RBF,\n",
                "    ConstantKernel,\n",
                "    DotProduct,\n",
                "    ExpSineSquared,\n",
                "    Kernel,\n",
                "    RationalQuadratic,\n",
                "    WhiteKernel,\n",
                ")\n",
                "from gluonts.dataset.arrow import ArrowWriter\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# --- KernelSynth Logic ---\n",
                "LENGTH = 1024\n",
                "KERNEL_BANK = [\n",
                "    ExpSineSquared(periodicity=24 / LENGTH),\n",
                "    ExpSineSquared(periodicity=7 / LENGTH),\n",
                "    RBF(length_scale=0.1),\n",
                "    RationalQuadratic(alpha=0.1),\n",
                "    WhiteKernel(noise_level=0.1),\n",
                "    ConstantKernel(),\n",
                "]\n",
                "\n",
                "def random_binary_map(a: Kernel, b: Kernel):\n",
                "    binary_maps = [lambda x, y: x + y, lambda x, y: x * y]\n",
                "    return np.random.choice(binary_maps)(a, b)\n",
                "\n",
                "def sample_from_gp_prior(kernel: Kernel, X: np.ndarray, random_seed: Optional[int] = None):\n",
                "    if X.ndim == 1:\n",
                "        X = X[:, None]\n",
                "    gpr = GaussianProcessRegressor(kernel=kernel)\n",
                "    ts = gpr.sample_y(X, n_samples=1, random_state=random_seed)\n",
                "    return ts\n",
                "\n",
                "def generate_time_series(max_kernels: int = 3):\n",
                "    while True:\n",
                "        X = np.linspace(0, 1, LENGTH)\n",
                "        selected_kernels = np.random.choice(\n",
                "            KERNEL_BANK, np.random.randint(1, max_kernels + 1), replace=True\n",
                "        )\n",
                "        kernel = functools.reduce(random_binary_map, selected_kernels)\n",
                "        try:\n",
                "            ts = sample_from_gp_prior(kernel=kernel, X=X)\n",
                "            return {\"start\": np.datetime64(\"2000-01-01 00:00\", \"s\"), \"target\": ts.squeeze()}\n",
                "        except Exception:\n",
                "            continue\n",
                "\n",
                "# Generate Data\n",
                "DATA_PATH = Path(\"kernelsynth-data.arrow\")\n",
                "if not DATA_PATH.exists():\n",
                "    print(\"Generating synthetic data...\")\n",
                "    # Ensure we use the global seed for generation logic if relying on np.random\n",
                "    # The set_seed(SEED) call above handles np.random.seed\n",
                "    generated_dataset = [\n",
                "        generate_time_series(max_kernels=3)\n",
                "        for _ in tqdm(range(1000)) # Generate 1000 series for baseline\n",
                "    ]\n",
                "    ArrowWriter(compression=\"lz4\").write_to_file(\n",
                "        generated_dataset,\n",
                "        path=DATA_PATH,\n",
                "    )\n",
                "    print(f\"Data saved to {DATA_PATH}\")\n",
                "else:\n",
                "    print(f\"Data already exists at {DATA_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Dataset Preparation\n",
                "\n",
                "We load the data and prepare it for `Chronos2Dataset`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data from Arrow file\n",
                "raw_dataset = FileDataset(DATA_PATH, freq=\"h\")\n",
                "\n",
                "# Convert to list of dicts with only 'target' (and optionally covariates if we had them)\n",
                "# Chronos2Dataset expects inputs to be a Sequence of Mappings\n",
                "data = [{\"target\": entry[\"target\"]} for entry in raw_dataset]\n",
                "\n",
                "# Split into train/validation (simple split)\n",
                "train_data = data[:900]\n",
                "val_data = data[900:]\n",
                "\n",
                "print(f\"Training samples: {len(train_data)}\")\n",
                "print(f\"Validation samples: {len(val_data)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Initialization & Training\n",
                "\n",
                "We initialize `Chronos2Model` with a \"tiny\" configuration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "CONTEXT_LENGTH = 512\n",
                "PREDICTION_LENGTH = 64\n",
                "PATCH_SIZE = 8 # Example patch size\n",
                "\n",
                "# Chronos-2 Forecasting Config\n",
                "chronos_forecasting_config = Chronos2ForecastingConfig(\n",
                "    context_length=CONTEXT_LENGTH,\n",
                "    output_patch_size=PATCH_SIZE,\n",
                "    input_patch_size=PATCH_SIZE,\n",
                "    input_patch_stride=PATCH_SIZE,\n",
                "    quantiles=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
                ")\n",
                "\n",
                "# Chronos-2 Core Config (Tiny)\n",
                "model_config = Chronos2CoreConfig(\n",
                "    d_model=256,\n",
                "    d_kv=32,\n",
                "    d_ff=1024,\n",
                "    num_layers=4,\n",
                "    num_heads=4,\n",
                "    dropout_rate=0.1,\n",
                "    vocab_size=2,\n",
                ")\n",
                "model_config.chronos_config = chronos_forecasting_config.__dict__\n",
                "\n",
                "# Initialize Model\n",
                "model = Chronos2Model(model_config)\n",
                "print(f\"Model Parameters: {model.num_parameters() / 1e6:.2f}M\")\n",
                "\n",
                "# Prepare Datasets\n",
                "train_ds = Chronos2Dataset(\n",
                "    inputs=train_data,\n",
                "    context_length=CONTEXT_LENGTH,\n",
                "    prediction_length=PREDICTION_LENGTH,\n",
                "    batch_size=32,\n",
                "    output_patch_size=PATCH_SIZE,\n",
                "    mode=DatasetMode.TRAIN,\n",
                ")\n",
                "\n",
                "val_ds = Chronos2Dataset(\n",
                "    inputs=val_data,\n",
                "    context_length=CONTEXT_LENGTH,\n",
                "    prediction_length=PREDICTION_LENGTH,\n",
                "    batch_size=32,\n",
                "    output_patch_size=PATCH_SIZE,\n",
                "    mode=DatasetMode.VALIDATION,\n",
                ")\n",
                "\n",
                "# Training Arguments\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=CHECKPOINT_DIR,\n",
                "    per_device_train_batch_size=32,\n",
                "    per_device_eval_batch_size=32,\n",
                "    learning_rate=1e-3,\n",
                "    num_train_epochs=5,\n",
                "    logging_steps=10,\n",
                "    save_strategy=\"epoch\",\n",
                "    evaluation_strategy=\"epoch\",\n",
                "    fp16=False, # Disable fp16 for T4 compatibility if needed, or check device support. \n",
                "    dataloader_num_workers=2,\n",
                "    remove_unused_columns=False,\n",
                "    report_to=\"wandb\", # Enable WandB tracking\n",
                "    run_name=\"chronos2-tiny-baseline\",\n",
                "    seed=SEED,\n",
                "    data_seed=SEED,\n",
                ")\n",
                "\n",
                "# Trainer\n",
                "trainer = Chronos2Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_ds,\n",
                "    eval_dataset=val_ds,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Run Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Starting training...\")\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Validation\n",
                "\n",
                "We perform a final validation to ensure the model is performing as expected."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Running final validation...\")\n",
                "eval_results = trainer.evaluate()\n",
                "print(f\"Final Validation Results: {eval_results}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}