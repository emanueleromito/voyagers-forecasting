{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Chronos-2 Training & Benchmarking\n",
                "\n",
                "This notebook establishes a training baseline for Chronos-2 using a mix of synthetic data and real-world datasets (Chronos datasets, GiftEval)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone Repository\n",
                "!git clone https://github.com/emanueleromito/voyagers-forecasting.git\n",
                "%cd voyagers-forecasting\n",
                "\n",
                "# Create checkpoint directory\n",
                "import os\n",
                "CHECKPOINT_DIR = './checkpoints'\n",
                "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -e .[dev]\n",
                "!pip install gluonts transformers accelerate typer typer-config rich wandb datasets\n",
                "\n",
                "# Fix SymPy compatibility issue with PyTorch\n",
                "!pip install --upgrade sympy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import random\n",
                "import torch\n",
                "import numpy as np\n",
                "import wandb\n",
                "import transformers\n",
                "import datasets\n",
                "import math\n",
                "from pathlib import Path\n",
                "from typing import Optional, List, Iterator, Sequence, Mapping, Any, Union\n",
                "from huggingface_hub import hf_hub_download\n",
                "from transformers import TrainingArguments\n",
                "from google.colab import userdata\n",
                "from gluonts.dataset.common import FileDataset\n",
                "from torch.utils.data import IterableDataset\n",
                "\n",
                "# Add src to path\n",
                "sys.path.append(os.path.abspath(\"src\"))\n",
                "\n",
                "# Chronos Imports\n",
                "from chronos2.config import Chronos2CoreConfig, Chronos2ForecastingConfig\n",
                "from chronos2.model import Chronos2Model\n",
                "from chronos2.dataset import Chronos2Dataset, DatasetMode, left_pad_and_cat_2D, validate_and_prepare_single_dict_task\n",
                "from chronos2.trainer import Chronos2Trainer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.1 Configuration & Hyperparameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Reproducibility ---\n",
                "SEED = 42\n",
                "DATA_PATH = Path(\"kernelsynth-data-paper.arrow\")\n",
                "\n",
                "# --- Model Configuration ---\n",
                "CONTEXT_LENGTH = 2048\n",
                "PREDICTION_LENGTH = 64\n",
                "PATCH_SIZE = 16\n",
                "D_MODEL = 192\n",
                "D_KV = 16\n",
                "D_FF = 768\n",
                "NUM_LAYERS = 3\n",
                "NUM_HEADS = 3\n",
                "DROPOUT_RATE = 0.1\n",
                "VOCAB_SIZE = 2\n",
                "QUANTILES = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99]\n",
                "\n",
                "# --- Training Configuration ---\n",
                "BATCH_SIZE = 32\n",
                "LEARNING_RATE = 1e-3\n",
                "MAX_STEPS = 10000\n",
                "SAVE_STEPS = 1000\n",
                "LOGGING_STEPS = 100\n",
                "WARMUP_RATIO = 0.0\n",
                "RUN_NAME = \"chronos2-baseline\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Reproducibility Setup ---\n",
                "def set_seed(seed: int = 42):\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.manual_seed_all(seed)\n",
                "        torch.backends.cudnn.deterministic = True\n",
                "        torch.backends.cudnn.benchmark = False\n",
                "    \n",
                "    print(f\"Random seed set to {seed}\")\n",
                "\n",
                "set_seed(SEED)\n",
                "\n",
                "# Check for GPU\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"\\nUsing GPU: {torch.cuda.get_device_name(0)}\")\n",
                "else:\n",
                "    print(\"\\nWARNING: GPU not available. Training will be slow.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wandb.login(key=userdata.get('wandb'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Dataset Implementation (Streaming)\n",
                "\n",
                "We implement a custom `StreamingChronosDataset` that handles mixing multiple streaming datasets and yielding batches directly, as required by `Chronos2Trainer`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class StreamingChronosDataset(IterableDataset):\n",
                "    def __init__(\n",
                "        self,\n",
                "        datasets: List[Any],\n",
                "        probabilities: List[float],\n",
                "        context_length: int,\n",
                "        prediction_length: int,\n",
                "        batch_size: int,\n",
                "        output_patch_size: int,\n",
                "        min_past: int = 1,\n",
                "        mode: str = DatasetMode.TRAIN,\n",
                "    ):\n",
                "        super().__init__()\n",
                "        assert len(datasets) == len(probabilities), f\"Number of datasets ({len(datasets)}) must match number of probabilities ({len(probabilities)})\"\n",
                "        self.datasets = datasets\n",
                "        self.probabilities = probabilities\n",
                "        self.context_length = context_length\n",
                "        self.prediction_length = prediction_length\n",
                "        self.batch_size = batch_size\n",
                "        self.output_patch_size = output_patch_size\n",
                "        self.min_past = min_past\n",
                "        self.mode = mode\n",
                "        self.max_output_patches = math.ceil(prediction_length / output_patch_size)\n",
                "\n",
                "    def _get_stream_iterators(self):\n",
                "        return [iter(ds) for ds in self.datasets]\n",
                "\n",
                "    def _construct_slice(self, task):\n",
                "        # task is a tuple returned by validate_and_prepare_single_dict_task\n",
                "        (\n",
                "            task_past_tensor,\n",
                "            task_future_tensor,\n",
                "            task_n_targets,\n",
                "            task_n_covariates,\n",
                "            task_n_future_covariates,\n",
                "        ) = task\n",
                "        \n",
                "        # Clone to avoid side effects if reused (though in streaming usually not reused)\n",
                "        task_past_tensor, task_future_tensor = task_past_tensor.clone(), task_future_tensor.clone()\n",
                "        task_n_past_only_covariates = task_n_covariates - task_n_future_covariates\n",
                "        full_length = task_past_tensor.shape[-1]\n",
                "\n",
                "        if self.mode == DatasetMode.TRAIN:\n",
                "            # slice a random subsequence\n",
                "            # Ensure we have enough history\n",
                "            if full_length < self.min_past + self.prediction_length:\n",
                "                 # This should have been filtered, but double check\n",
                "                 return None\n",
                "            slice_idx = np.random.randint(self.min_past, full_length - self.prediction_length + 1)\n",
                "        elif self.mode == DatasetMode.VALIDATION:\n",
                "            slice_idx = full_length - self.prediction_length\n",
                "        else:\n",
                "            slice_idx = full_length\n",
                "\n",
                "        if slice_idx >= self.context_length:\n",
                "            task_context = task_past_tensor[:, slice_idx - self.context_length : slice_idx]\n",
                "        else:\n",
                "            task_context = task_past_tensor[:, :slice_idx]\n",
                "\n",
                "        if self.mode in [DatasetMode.TRAIN, DatasetMode.VALIDATION]:\n",
                "            task_future_target = task_past_tensor[:, slice_idx : slice_idx + self.prediction_length].clone()\n",
                "            task_future_target[task_n_targets:] = torch.nan\n",
                "\n",
                "            if task_n_future_covariates > 0:\n",
                "                task_future_covariates = task_past_tensor[\n",
                "                    -task_n_future_covariates:, slice_idx : slice_idx + self.prediction_length\n",
                "                ]\n",
                "            else:\n",
                "                task_future_covariates = torch.zeros((0, self.prediction_length))\n",
                "\n",
                "            task_future_covariates_padding = torch.full(\n",
                "                (task_n_targets + task_n_past_only_covariates, self.prediction_length),\n",
                "                fill_value=torch.nan,\n",
                "            )\n",
                "            task_future_covariates = torch.cat([task_future_covariates_padding, task_future_covariates], dim=0)\n",
                "        else:\n",
                "            task_future_target = None\n",
                "            task_future_covariates = task_future_tensor\n",
                "\n",
                "        return task_context, task_future_target, task_future_covariates, task_n_targets\n",
                "\n",
                "    def _build_batch(self, batch_samples):\n",
                "        batch_context_tensor_list = []\n",
                "        batch_future_target_tensor_list = []\n",
                "        batch_future_covariates_tensor_list = []\n",
                "        batch_group_ids_list = []\n",
                "        target_idx_ranges = []\n",
                "\n",
                "        target_start_idx = 0\n",
                "        for group_id, sample in enumerate(batch_samples):\n",
                "            task_context, task_future_target, task_future_covariates, task_n_targets = sample\n",
                "\n",
                "            group_size = task_context.shape[0]\n",
                "            task_group_ids = torch.full((group_size,), fill_value=group_id)\n",
                "            batch_context_tensor_list.append(task_context)\n",
                "            batch_future_target_tensor_list.append(task_future_target)\n",
                "            batch_future_covariates_tensor_list.append(task_future_covariates)\n",
                "            batch_group_ids_list.append(task_group_ids)\n",
                "            target_idx_ranges.append((target_start_idx, target_start_idx + task_n_targets))\n",
                "            target_start_idx += group_size\n",
                "\n",
                "        if self.mode == DatasetMode.TRAIN:\n",
                "            num_output_patches = np.random.randint(1, self.max_output_patches + 1)\n",
                "        else:\n",
                "            num_output_patches = self.max_output_patches\n",
                "            \n",
                "        horizon = num_output_patches * self.output_patch_size\n",
                "\n",
                "        future_target = None\n",
                "        if self.mode != DatasetMode.TEST:\n",
                "            future_target = torch.cat(batch_future_target_tensor_list, dim=0)\n",
                "            if future_target.shape[-1] > horizon:\n",
                "                future_target = future_target[..., :horizon]\n",
                "\n",
                "        future_covariates = torch.cat(batch_future_covariates_tensor_list, dim=0)\n",
                "        if future_covariates.shape[-1] > horizon:\n",
                "            future_covariates = future_covariates[..., :horizon]\n",
                "\n",
                "        return {\n",
                "            \"context\": left_pad_and_cat_2D(batch_context_tensor_list),\n",
                "            \"future_target\": future_target,\n",
                "            \"future_covariates\": future_covariates,\n",
                "            \"group_ids\": torch.cat(batch_group_ids_list, dim=0),\n",
                "            \"num_output_patches\": num_output_patches,\n",
                "            \"target_idx_ranges\": target_idx_ranges,\n",
                "        }\n",
                "\n",
                "    def __iter__(self):\n",
                "        iterators = self._get_stream_iterators()\n",
                "        batch_samples = []\n",
                "        current_batch_size = 0\n",
                "\n",
                "        while True:\n",
                "            # Sample a dataset\n",
                "            idx = np.random.choice(len(self.datasets), p=self.probabilities)\n",
                "            try:\n",
                "                raw_entry = next(iterators[idx])\n",
                "            except StopIteration:\n",
                "                # Restart iterator if exhausted (cyclic)\n",
                "                iterators[idx] = iter(self.datasets[idx])\n",
                "                try:\n",
                "                    raw_entry = next(iterators[idx])\n",
                "                except StopIteration:\n",
                "                    # Dataset is empty or cannot be restarted, skip it\n",
                "                    continue\n",
                "\n",
                "            # Prepare task\n",
                "            # We need to adapt raw_entry to what validate_and_prepare_single_dict_task expects\n",
                "            # It expects dict with 'target' (numpy/tensor), 'past_covariates', etc.\n",
                "            # Our adapter should have already ensured this format.\n",
                "            \n",
                "            try:\n",
                "                # Validate and prepare\n",
                "                # We pass idx=0 as it's single task processing\n",
                "                task = validate_and_prepare_single_dict_task(raw_entry, idx=0, prediction_length=self.prediction_length)\n",
                "                \n",
                "                # Filter short series\n",
                "                if self.mode != DatasetMode.TEST and task[0].shape[-1] < self.min_past + self.prediction_length:\n",
                "                    continue\n",
                "\n",
                "                # Construct slice\n",
                "                sample = self._construct_slice(task)\n",
                "                if sample is None:\n",
                "                    continue\n",
                "\n",
                "                batch_samples.append(sample)\n",
                "                current_batch_size += sample[0].shape[0] # Add group size (number of series in task)\n",
                "\n",
                "                if current_batch_size >= self.batch_size:\n",
                "                    batch = self._build_batch(batch_samples)\n",
                "                    \n",
                "                    # Remove target_idx_ranges for training/validation as model.forward doesn't accept it\n",
                "                    if self.mode in [DatasetMode.TRAIN, DatasetMode.VALIDATION]:\n",
                "                        batch.pop(\"target_idx_ranges\")\n",
                "                        \n",
                "                    yield batch\n",
                "                    batch_samples = []\n",
                "                    current_batch_size = 0\n",
                "\n",
                "            except Exception as e:\n",
                "                # Skip bad samples\n",
                "                # print(f\"Error processing sample: {e}\")\n",
                "                continue"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Loading & Adapters\n",
                "\n",
                "We load all datasets listed in the Chronos-2 paper and adapt them to the required format."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Adapter for HF Datasets\n",
                "class HFDatasetAdapter:\n",
                "    def __init__(self, hf_dataset, target_column=\"target\", name=\"unknown\"):\n",
                "        self.hf_dataset = hf_dataset\n",
                "        self.target_column = target_column\n",
                "        self.name = name\n",
                "\n",
                "    def __iter__(self):\n",
                "        for entry in self.hf_dataset:\n",
                "            target_col = self.target_column\n",
                "            \n",
                "            if target_col not in entry:\n",
                "                # Try to find target column\n",
                "                keys = list(entry.keys())\n",
                "                \n",
                "                # Prioritize known names\n",
                "                candidates = [\"series\", \"values\", \"ts\", \"target\"]\n",
                "                found = False\n",
                "                for cand in candidates:\n",
                "                    if cand in keys:\n",
                "                        target_col = cand\n",
                "                        found = True\n",
                "                        break\n",
                "                \n",
                "                if not found:\n",
                "                    # Fallback: look for a list/array column that is NOT a timestamp\n",
                "                    # and contains numbers.\n",
                "                    for k, v in entry.items():\n",
                "                        if k in [\"start\", \"timestamp\", \"date\", \"id\", \"item_id\", \"feat_static_cat\", \"feat_dynamic_real\"]:\n",
                "                            continue\n",
                "                        \n",
                "                        # Check if value is a sequence\n",
                "                        if isinstance(v, (list, np.ndarray)):\n",
                "                            # Check content type if not empty\n",
                "                            if len(v) > 0:\n",
                "                                first_elem = v[0]\n",
                "                                if isinstance(first_elem, (int, float, np.number)):\n",
                "                                    target_col = k\n",
                "                                    found = True\n",
                "                                    break\n",
                "            \n",
                "            if target_col not in entry:\n",
                "                 # print(f\"Warning: Could not find target column in dataset {self.name}. Available keys: {list(entry.keys())}\")\n",
                "                 continue\n",
                "\n",
                "            try:\n",
                "                val = entry[target_col]\n",
                "                # Ensure it is a sequence of numbers\n",
                "                if isinstance(val, (list, np.ndarray)):\n",
                "                     # Check first element again to be safe against list of datetimes\n",
                "                     if len(val) > 0 and not isinstance(val[0], (int, float, np.number)):\n",
                "                         # Skip if not numeric\n",
                "                         continue\n",
                "                             \n",
                "                     target = np.array(val, dtype=np.float32)\n",
                "                     yield {\"target\": target}\n",
                "            except (ValueError, TypeError):\n",
                "                # print(f\"Error converting target column '{target_col}' in dataset {self.name}\")\n",
                "                continue\n",
                "\n",
                "# Load Synthetic Dataset\n",
                "print(\"Downloading synthetic dataset...\")\n",
                "HF_REPO_ID = \"voyagersnlppolito/model-data\"\n",
                "dataset_path = hf_hub_download(\n",
                "    repo_id=HF_REPO_ID,\n",
                "    filename=\"synthetic_dataset.arrow\",\n",
                "    repo_type=\"dataset\",\n",
                "    token=userdata.get('HF_TOKEN')\n",
                ")\n",
                "synthetic_ds = FileDataset(path=Path(dataset_path), freq=\"h\")\n",
                "\n",
                "# List of Chronos-2 Datasets (Autogluon)\n",
                "# Mapping based on standard names in autogluon/chronos_datasets\n",
                "chronos_dataset_names = [\n",
                "    \"dominick\",\n",
                "    \"electricity_15min\",\n",
                "    \"ercot\",\n",
                "    \"exchange_rate\",\n",
                "    \"m4_daily\",\n",
                "    \"m4_hourly\",\n",
                "    \"m4_monthly\",\n",
                "    \"m4_quarterly\",\n",
                "    \"m4_weekly\",\n",
                "    \"m4_yearly\",\n",
                "    \"m5\",\n",
                "    \"mexico_city_bikes\",\n",
                "    \"monash_australian_electricity\",\n",
                "    \"monash_car_parts\",\n",
                "    \"monash_cif_2016\",\n",
                "    \"monash_covid_deaths\",\n",
                "    \"monash_electricity_hourly\",\n",
                "    \"monash_electricity_weekly\",\n",
                "    \"monash_fred_md\",\n",
                "    \"monash_hospital\",\n",
                "    \"monash_kdd_cup_2018\",\n",
                "    \"monash_london_smart_meters\",\n",
                "    \"monash_m1_monthly\",\n",
                "    \"monash_m1_quarterly\",\n",
                "    \"monash_m1_yearly\",\n",
                "    \"monash_m3_monthly\",\n",
                "    \"monash_m3_quarterly\",\n",
                "    \"monash_m3_yearly\",\n",
                "    \"monash_nn5_weekly\",\n",
                "    \"monash_pedestrian_counts\",\n",
                "    \"monash_rideshare\",\n",
                "    \"monash_saugeenday\",\n",
                "    \"monash_temperature_rain\",\n",
                "    \"monash_tourism_monthly\",\n",
                "    \"monash_tourism_quarterly\",\n",
                "    \"monash_tourism_yearly\",\n",
                "    \"monash_traffic\",\n",
                "    \"monash_weather\",\n",
                "    \"nn5\",\n",
                "    \"solar\",\n",
                "    \"solar_1h\",\n",
                "    \"taxi_1h\",\n",
                "    \"taxi_30min\",\n",
                "    \"uber_tlc_daily\",\n",
                "    \"uber_tlc_hourly\",\n",
                "    \"ushcn_daily\",\n",
                "    \"weatherbench_daily\",\n",
                "    \"weatherbench_hourly_10m_u_component_of_wind\",\n",
                "    \"weatherbench_hourly_10m_v_component_of_wind\",\n",
                "    \"weatherbench_hourly_2m_temperature\",\n",
                "    \"weatherbench_hourly_geopotential\",\n",
                "    \"weatherbench_hourly_potential_vorticity\",\n",
                "    \"weatherbench_hourly_relative_humidity\",\n",
                "    \"weatherbench_hourly_specific_humidity\",\n",
                "    \"weatherbench_hourly_temperature\",\n",
                "    \"weatherbench_hourly_toa_incident_solar_radiation\",\n",
                "    \"weatherbench_hourly_total_cloud_cover\",\n",
                "    \"weatherbench_hourly_total_precipitation\",\n",
                "    \"weatherbench_hourly_u_component_of_wind\",\n",
                "    \"weatherbench_hourly_v_component_of_wind\",\n",
                "    \"weatherbench_hourly_vorticity\",\n",
                "    \"weatherbench_weekly\",\n",
                "    \"wiki_daily_100k\",\n",
                "    \"wind_farms_daily\",\n",
                "    \"wind_farms_hourly\",\n",
                "]\n",
                "\n",
                "# List of GiftEval Datasets (Salesforce/GiftEvalPretrain)\n",
                "gifteval_dataset_names = [\n",
                "    \"BEIJING_SUBWAY_30MIN\",\n",
                "    \"HZMETRO\",\n",
                "    \"LOS_LOOP\",\n",
                "    \"PEMS03\",\n",
                "    \"PEMS04\",\n",
                "    \"PEMS07\",\n",
                "    \"PEMS08\",\n",
                "    \"PEMS_BAY\",\n",
                "    \"Q-TRAFFIC\",\n",
                "    \"SHMETRO\",\n",
                "    \"alibaba_cluster_trace_2018\",\n",
                "    \"australian_electricity_demand\",\n",
                "    \"azure_vm_traces_2017\",\n",
                "    \"bdg-2_bear\",\n",
                "    \"bdg-2_fox\",\n",
                "    \"bdg-2_panther\",\n",
                "    \"bdg-2_rat\",\n",
                "    \"beijing_air_quality\",\n",
                "    \"bitcoin_with_missing\",\n",
                "    \"borealis\",\n",
                "    \"borg_cluster_data_2011\",\n",
                "    \"buildings_900k\",\n",
                "    \"bull\",\n",
                "    \"cdc_fluview_ilinet\",\n",
                "    \"cdc_fluview_who_nrevss\",\n",
                "    \"china_air_quality\",\n",
                "    \"cif_2016_12\",\n",
                "    \"cif_2016_6\",\n",
                "    \"cockatoo\",\n",
                "    \"covid19_energy\",\n",
                "    \"covid_mobility\",\n",
                "    \"elecdemand\",\n",
                "    \"elf\",\n",
                "    \"extended_web_traffic_with_missing\",\n",
                "    \"favorita_sales\",\n",
                "    \"favorita_transactions\",\n",
                "    \"fred_md\",\n",
                "    \"gfc12_load\",\n",
                "    \"gfc14_load\",\n",
                "    \"gfc17_load\",\n",
                "    \"godaddy\",\n",
                "    \"hog\",\n",
                "    \"ideal\",\n",
                "    \"kaggle_web_traffic_weekly\",\n",
                "    \"kdd2022\",\n",
                "    \"largest_2017\",\n",
                "    \"largest_2018\",\n",
                "    \"largest_2019\",\n",
                "    \"largest_2020\",\n",
                "    \"largest_2021\",\n",
                "    \"lcl\",\n",
                "    \"london_smart_meters_with_missing\",\n",
                "    \"m1_monthly\",\n",
                "    \"m1_quarterly\",\n",
                "    \"m1_yearly\",\n",
                "    \"m5\",\n",
                "    \"monash_m3_monthly\",\n",
                "    \"monash_m3_other\",\n",
                "    \"monash_m3_quarterly\",\n",
                "    \"monash_m3_yearly\",\n",
                "    \"nn5_daily_with_missing\",\n",
                "    \"nn5_weekly\",\n",
                "    \"oikolab_weather\",\n",
                "    \"pdb\",\n",
                "    \"pedestrian_counts\",\n",
                "    \"project_tycho\",\n",
                "    \"residential_load_power\",\n",
                "    \"residential_pv_power\",\n",
                "    \"rideshare_with_missing\",\n",
                "    \"sceaux\",\n",
                "    \"smart\",\n",
                "    \"solar_power\",\n",
                "    \"spain\",\n",
                "    \"subseasonal\",\n",
                "    \"subseasonal_precip\",\n",
                "    \"sunspot_with_missing\",\n",
                "    \"taxi_30min\",\n",
                "    \"tourism_monthly\",\n",
                "    \"tourism_quarterly\",\n",
                "    \"tourism_yearly\",\n",
                "    \"traffic_hourly\",\n",
                "    \"traffic_weekly\",\n",
                "    \"uber_tlc_daily\",\n",
                "    \"uber_tlc_hourly\",\n",
                "    \"vehicle_trips_with_missing\",\n",
                "    \"weather\",\n",
                "    \"wiki-rolling_nips\",\n",
                "    \"wind_farms_with_missing\",\n",
                "    \"wind_power\",\n",
                "]\n",
                "\n",
                "# Add CMIP6 and ERA5 datasets (ranges)\n",
                "for year in range(1850, 2011, 5):\n",
                "    gifteval_dataset_names.append(f\"cmip6_{year}\")\n",
                "for year in range(1989, 2019):\n",
                "    gifteval_dataset_names.append(f\"era5_{year}\")\n",
                "\n",
                "datasets_list = []\n",
                "datasets_list.append(synthetic_ds)\n",
                "\n",
                "print(\"Loading Chronos datasets...\")\n",
                "for name in chronos_dataset_names:\n",
                "    try:\n",
                "        ds = datasets.load_dataset(\"autogluon/chronos_datasets\", name, split=\"train\", streaming=True)\n",
                "        datasets_list.append(HFDatasetAdapter(ds, name=name))\n",
                "        print(f\"Loaded {name}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Could not load {name}: {e}\")\n",
                "\n",
                "print(\"Loading GiftEval datasets...\")\n",
                "for name in gifteval_dataset_names:\n",
                "    try:\n",
                "        # Try loading with data_dir for GiftEval as they don't seem to be named configs\n",
                "        ds = datasets.load_dataset(\"Salesforce/GiftEvalPretrain\", split=\"train\", data_dir=name, streaming=True)\n",
                "        datasets_list.append(HFDatasetAdapter(ds, name=name))\n",
                "        print(f\"Loaded {name}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Could not load {name}: {e}\")\n",
                "\n",
                "# Calculate probabilities (Uniform for now, or weighted by size if known)\n",
                "# To fix the ValueError, we ensure len(probs) == len(datasets)\n",
                "num_datasets = len(datasets_list)\n",
                "probabilities = [1.0 / num_datasets] * num_datasets\n",
                "\n",
                "print(f\"Total datasets loaded: {num_datasets}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training Execution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create Streaming Datasets\n",
                "train_ds = StreamingChronosDataset(\n",
                "    datasets=dataset_list,\n",
                "    probabilities=probabilities,\n",
                "    context_length=CONTEXT_LENGTH,\n",
                "    prediction_length=PREDICTION_LENGTH,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    output_patch_size=PATCH_SIZE,\n",
                "    mode=DatasetMode.TRAIN,\n",
                ")\n",
                "\n",
                "# Validation (use synthetic only for speed/stability)\n",
                "val_ds = StreamingChronosDataset(\n",
                "    datasets=[synthetic_ds],\n",
                "    probabilities=[1.0],\n",
                "    context_length=CONTEXT_LENGTH,\n",
                "    prediction_length=PREDICTION_LENGTH,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    output_patch_size=PATCH_SIZE,\n",
                "    mode=DatasetMode.VALIDATION,\n",
                ")\n",
                "\n",
                "# Training Arguments\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=CHECKPOINT_DIR,\n",
                "    per_device_train_batch_size=BATCH_SIZE,\n",
                "    per_device_eval_batch_size=BATCH_SIZE,\n",
                "    learning_rate=LEARNING_RATE,\n",
                "    lr_scheduler_type=\"linear\",\n",
                "    warmup_ratio=WARMUP_RATIO,\n",
                "    max_steps=MAX_STEPS,\n",
                "    save_steps=SAVE_STEPS,\n",
                "    logging_steps=LOGGING_STEPS,\n",
                "    save_strategy=\"steps\",\n",
                "    fp16=False,\n",
                "    dataloader_num_workers=0, # Enable workers for parallel loading\n",
                "    dataloader_pin_memory=False, # Enable pin memory\n",
                "    remove_unused_columns=False,\n",
                "    report_to=\"wandb\",\n",
                "    run_name=RUN_NAME,\n",
                "    seed=SEED,\n",
                "    data_seed=SEED,\n",
                ")\n",
                "\n",
                "# Initialize Model (Same as before)\n",
                "chronos_forecasting_config = Chronos2ForecastingConfig(\n",
                "    context_length=CONTEXT_LENGTH,\n",
                "    output_patch_size=PATCH_SIZE,\n",
                "    input_patch_size=PATCH_SIZE,\n",
                "    input_patch_stride=PATCH_SIZE,\n",
                "    quantiles=QUANTILES,\n",
                "    time_encoding_scale=CONTEXT_LENGTH,\n",
                "    use_reg_token=True,\n",
                "    use_arcsinh=True,\n",
                "    max_output_patches=64,\n",
                ")\n",
                "\n",
                "model_config = Chronos2CoreConfig(\n",
                "    d_model=D_MODEL,\n",
                "    d_kv=D_KV,\n",
                "    d_ff=D_FF,\n",
                "    num_layers=NUM_LAYERS,\n",
                "    num_heads=NUM_HEADS,\n",
                "    dropout_rate=DROPOUT_RATE,\n",
                "    vocab_size=VOCAB_SIZE,\n",
                ")\n",
                "model_config.chronos_config = chronos_forecasting_config.__dict__\n",
                "model = Chronos2Model(model_config)\n",
                "\n",
                "# Trainer\n",
                "trainer = Chronos2Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_ds,\n",
                "    eval_dataset=val_ds,\n",
                ")\n",
                "\n",
                "print(\"Starting training...\")\n",
                "trainer.train()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
