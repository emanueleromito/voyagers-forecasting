{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Chronos-2 Training & Benchmarking\n",
                "\n",
                "This notebook establishes a training baseline for Chronos-2 using a mix of synthetic data and real-world datasets (Chronos datasets, GiftEval)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone Repository\n",
                "!git clone https://github.com/emanueleromito/voyagers-forecasting.git\n",
                "%cd voyagers-forecasting\n",
                "\n",
                "# Create checkpoint directory\n",
                "import os\n",
                "CHECKPOINT_DIR = './checkpoints'\n",
                "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -e .[dev]\n",
                "!pip install gluonts transformers accelerate typer typer-config rich wandb datasets\n",
                "\n",
                "# Fix SymPy compatibility issue with PyTorch\n",
                "!pip install --upgrade sympy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import random\n",
                "import torch\n",
                "import numpy as np\n",
                "import wandb\n",
                "import transformers\n",
                "import datasets\n",
                "import math\n",
                "from pathlib import Path\n",
                "from typing import Optional, List, Iterator, Sequence, Mapping, Any, Union\n",
                "from huggingface_hub import hf_hub_download\n",
                "from transformers import TrainingArguments\n",
                "from google.colab import userdata\n",
                "from gluonts.dataset.common import FileDataset\n",
                "from torch.utils.data import IterableDataset\n",
                "\n",
                "# Add src to path\n",
                "sys.path.append(os.path.abspath(\"src\"))\n",
                "\n",
                "# Chronos Imports\n",
                "from chronos2.config import Chronos2CoreConfig, Chronos2ForecastingConfig\n",
                "from chronos2.model import Chronos2Model\n",
                "from chronos2.dataset import Chronos2Dataset, DatasetMode, left_pad_and_cat_2D, validate_and_prepare_single_dict_task\n",
                "from chronos2.trainer import Chronos2Trainer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.1 Configuration & Hyperparameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Reproducibility ---\n",
                "SEED = 42\n",
                "DATA_PATH = Path(\"kernelsynth-data-paper.arrow\")\n",
                "\n",
                "# --- Model Configuration ---\n",
                "CONTEXT_LENGTH = 2048\n",
                "PREDICTION_LENGTH = 64\n",
                "PATCH_SIZE = 16\n",
                "D_MODEL = 192\n",
                "D_KV = 16\n",
                "D_FF = 768\n",
                "NUM_LAYERS = 3\n",
                "NUM_HEADS = 3\n",
                "DROPOUT_RATE = 0.1\n",
                "VOCAB_SIZE = 2\n",
                "QUANTILES = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99]\n",
                "\n",
                "# --- Training Configuration ---\n",
                "BATCH_SIZE = 32\n",
                "LEARNING_RATE = 1e-3\n",
                "MAX_STEPS = 10000\n",
                "SAVE_STEPS = 1000\n",
                "LOGGING_STEPS = 100\n",
                "WARMUP_RATIO = 0.0\n",
                "RUN_NAME = \"chronos2-baseline\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Reproducibility Setup ---\n",
                "def set_seed(seed: int = 42):\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.manual_seed_all(seed)\n",
                "        torch.backends.cudnn.deterministic = True\n",
                "        torch.backends.cudnn.benchmark = False\n",
                "    \n",
                "    print(f\"Random seed set to {seed}\")\n",
                "\n",
                "set_seed(SEED)\n",
                "\n",
                "# Check for GPU\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"\\nUsing GPU: {torch.cuda.get_device_name(0)}\")\n",
                "else:\n",
                "    print(\"\\nWARNING: GPU not available. Training will be slow.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wandb.login(key=userdata.get('wandb'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Dataset Implementation (Streaming)\n",
                "\n",
                "We implement a custom `StreamingChronosDataset` that handles mixing multiple streaming datasets and yielding batches directly, as required by `Chronos2Trainer`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class StreamingChronosDataset(IterableDataset):\n",
                "    def __init__(\n",
                "        self,\n",
                "        datasets: List[Any],\n",
                "        probabilities: List[float],\n",
                "        context_length: int,\n",
                "        prediction_length: int,\n",
                "        batch_size: int,\n",
                "        output_patch_size: int,\n",
                "        min_past: int = 1,\n",
                "        mode: str = DatasetMode.TRAIN,\n",
                "    ):\n",
                "        super().__init__()\n",
                "        assert len(datasets) == len(probabilities), f\"Number of datasets ({len(datasets)}) must match number of probabilities ({len(probabilities)})\"\n",
                "        self.datasets = datasets\n",
                "        self.probabilities = probabilities\n",
                "        self.context_length = context_length\n",
                "        self.prediction_length = prediction_length\n",
                "        self.batch_size = batch_size\n",
                "        self.output_patch_size = output_patch_size\n",
                "        self.min_past = min_past\n",
                "        self.mode = mode\n",
                "        self.max_output_patches = math.ceil(prediction_length / output_patch_size)\n",
                "\n",
                "    def _get_stream_iterators(self):\n",
                "        return [iter(ds) for ds in self.datasets]\n",
                "\n",
                "    def _construct_slice(self, task):\n",
                "        # task is a tuple returned by validate_and_prepare_single_dict_task\n",
                "        (\n",
                "            task_past_tensor,\n",
                "            task_future_tensor,\n",
                "            task_n_targets,\n",
                "            task_n_covariates,\n",
                "            task_n_future_covariates,\n",
                "        ) = task\n",
                "        \n",
                "        # Clone to avoid side effects if reused (though in streaming usually not reused)\n",
                "        task_past_tensor, task_future_tensor = task_past_tensor.clone(), task_future_tensor.clone()\n",
                "        task_n_past_only_covariates = task_n_covariates - task_n_future_covariates\n",
                "        full_length = task_past_tensor.shape[-1]\n",
                "\n",
                "        if self.mode == DatasetMode.TRAIN:\n",
                "            # slice a random subsequence\n",
                "            # Ensure we have enough history\n",
                "            if full_length < self.min_past + self.prediction_length:\n",
                "                 # This should have been filtered, but double check\n",
                "                 return None\n",
                "            slice_idx = np.random.randint(self.min_past, full_length - self.prediction_length + 1)\n",
                "        elif self.mode == DatasetMode.VALIDATION:\n",
                "            slice_idx = full_length - self.prediction_length\n",
                "        else:\n",
                "            slice_idx = full_length\n",
                "\n",
                "        if slice_idx >= self.context_length:\n",
                "            task_context = task_past_tensor[:, slice_idx - self.context_length : slice_idx]\n",
                "        else:\n",
                "            task_context = task_past_tensor[:, :slice_idx]\n",
                "\n",
                "        if self.mode in [DatasetMode.TRAIN, DatasetMode.VALIDATION]:\n",
                "            task_future_target = task_past_tensor[:, slice_idx : slice_idx + self.prediction_length].clone()\n",
                "            task_future_target[task_n_targets:] = torch.nan\n",
                "\n",
                "            if task_n_future_covariates > 0:\n",
                "                task_future_covariates = task_past_tensor[\n",
                "                    -task_n_future_covariates:, slice_idx : slice_idx + self.prediction_length\n",
                "                ]\n",
                "            else:\n",
                "                task_future_covariates = torch.zeros((0, self.prediction_length))\n",
                "\n",
                "            task_future_covariates_padding = torch.full(\n",
                "                (task_n_targets + task_n_past_only_covariates, self.prediction_length),\n",
                "                fill_value=torch.nan,\n",
                "            )\n",
                "            task_future_covariates = torch.cat([task_future_covariates_padding, task_future_covariates], dim=0)\n",
                "        else:\n",
                "            task_future_target = None\n",
                "            task_future_covariates = task_future_tensor\n",
                "\n",
                "        return task_context, task_future_target, task_future_covariates, task_n_targets\n",
                "\n",
                "    def _build_batch(self, batch_samples):\n",
                "        batch_context_tensor_list = []\n",
                "        batch_future_target_tensor_list = []\n",
                "        batch_future_covariates_tensor_list = []\n",
                "        batch_group_ids_list = []\n",
                "        target_idx_ranges = []\n",
                "\n",
                "        target_start_idx = 0\n",
                "        for group_id, sample in enumerate(batch_samples):\n",
                "            task_context, task_future_target, task_future_covariates, task_n_targets = sample\n",
                "\n",
                "            group_size = task_context.shape[0]\n",
                "            task_group_ids = torch.full((group_size,), fill_value=group_id)\n",
                "            batch_context_tensor_list.append(task_context)\n",
                "            batch_future_target_tensor_list.append(task_future_target)\n",
                "            batch_future_covariates_tensor_list.append(task_future_covariates)\n",
                "            batch_group_ids_list.append(task_group_ids)\n",
                "            target_idx_ranges.append((target_start_idx, target_start_idx + task_n_targets))\n",
                "            target_start_idx += group_size\n",
                "\n",
                "        if self.mode == DatasetMode.TRAIN:\n",
                "            num_output_patches = np.random.randint(1, self.max_output_patches + 1)\n",
                "        else:\n",
                "            num_output_patches = self.max_output_patches\n",
                "            \n",
                "        horizon = num_output_patches * self.output_patch_size\n",
                "\n",
                "        future_target = None\n",
                "        if self.mode != DatasetMode.TEST:\n",
                "            future_target = torch.cat(batch_future_target_tensor_list, dim=0)\n",
                "            if future_target.shape[-1] > horizon:\n",
                "                future_target = future_target[..., :horizon]\n",
                "\n",
                "        future_covariates = torch.cat(batch_future_covariates_tensor_list, dim=0)\n",
                "        if future_covariates.shape[-1] > horizon:\n",
                "            future_covariates = future_covariates[..., :horizon]\n",
                "\n",
                "        return {\n",
                "            \"context\": left_pad_and_cat_2D(batch_context_tensor_list),\n",
                "            \"future_target\": future_target,\n",
                "            \"future_covariates\": future_covariates,\n",
                "            \"group_ids\": torch.cat(batch_group_ids_list, dim=0),\n",
                "            \"num_output_patches\": num_output_patches,\n",
                "            \"target_idx_ranges\": target_idx_ranges,\n",
                "        }\n",
                "\n",
                "    def __iter__(self):\n",
                "        iterators = self._get_stream_iterators()\n",
                "        batch_samples = []\n",
                "        current_batch_size = 0\n",
                "\n",
                "        while True:\n",
                "            # Sample a dataset\n",
                "            idx = np.random.choice(len(self.datasets), p=self.probabilities)\n",
                "            try:\n",
                "                raw_entry = next(iterators[idx])\n",
                "            except StopIteration:\n",
                "                # Restart iterator if exhausted (cyclic)\n",
                "                iterators[idx] = iter(self.datasets[idx])\n",
                "                try:\n",
                "                    raw_entry = next(iterators[idx])\n",
                "                except StopIteration:\n",
                "                    # Dataset is empty or cannot be restarted, skip it\n",
                "                    continue\n",
                "\n",
                "            # Prepare task\n",
                "            # We need to adapt raw_entry to what validate_and_prepare_single_dict_task expects\n",
                "            # It expects dict with 'target' (numpy/tensor), 'past_covariates', etc.\n",
                "            # Our adapter should have already ensured this format.\n",
                "            \n",
                "            try:\n",
                "                # Validate and prepare\n",
                "                # We pass idx=0 as it's single task processing\n",
                "                task = validate_and_prepare_single_dict_task(raw_entry, idx=0, prediction_length=self.prediction_length)\n",
                "                \n",
                "                # Filter short series\n",
                "                if self.mode != DatasetMode.TEST and task[0].shape[-1] < self.min_past + self.prediction_length:\n",
                "                    continue\n",
                "\n",
                "                # Construct slice\n",
                "                sample = self._construct_slice(task)\n",
                "                if sample is None:\n",
                "                    continue\n",
                "\n",
                "                batch_samples.append(sample)\n",
                "                current_batch_size += sample[0].shape[0] # Add group size (number of series in task)\n",
                "\n",
                "                if current_batch_size >= self.batch_size:\n",
                "                    batch = self._build_batch(batch_samples)\n",
                "                    \n",
                "                    # Remove target_idx_ranges for training/validation as model.forward doesn't accept it\n",
                "                    if self.mode in [DatasetMode.TRAIN, DatasetMode.VALIDATION]:\n",
                "                        batch.pop(\"target_idx_ranges\")\n",
                "                        \n",
                "                    yield batch\n",
                "                    batch_samples = []\n",
                "                    current_batch_size = 0\n",
                "\n",
                "            except Exception as e:\n",
                "                # Skip bad samples\n",
                "                # print(f\"Error processing sample: {e}\")\n",
                "                continue"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Loading & Adapters\n",
                "\n",
                "We load all datasets listed in the Chronos-2 paper and adapt them to the required format."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Adapter for HF Datasets\n",
                "class HFDatasetAdapter:\n",
                "    def __init__(self, hf_dataset, target_column=\"target\", start_column=\"start\"):\n",
                "        self.hf_dataset = hf_dataset\n",
                "        self.target_column = target_column\n",
                "        self.start_column = start_column\n",
                "\n",
                "    def __iter__(self):\n",
                "        for entry in self.hf_dataset:\n",
                "            target = np.array(entry[self.target_column], dtype=np.float32)\n",
                "            # Chronos expects 'target' key\n",
                "            yield {\"target\": target}\n",
                "\n",
                "# Load Synthetic Dataset\n",
                "print(\"Downloading synthetic dataset...\")\n",
                "HF_REPO_ID = \"voyagersnlppolito/model-data\"\n",
                "dataset_path = hf_hub_download(\n",
                "    repo_id=HF_REPO_ID,\n",
                "    filename=\"synthetic_dataset.arrow\",\n",
                "    repo_type=\"dataset\",\n",
                "    token=userdata.get('HF_TOKEN')\n",
                ")\n",
                "# Load via datasets library to allow splitting\n",
                "synthetic_hf = datasets.load_dataset(\"arrow\", data_files=str(dataset_path), split=\"train\")\n",
                "synthetic_split = synthetic_hf.train_test_split(test_size=0.1, seed=SEED)\n",
                "synthetic_train = synthetic_split[\"train\"]\n",
                "synthetic_val = synthetic_split[\"test\"]\n",
                "\n",
                "# List of Chronos-2 Datasets (Autogluon)\n",
                "chronos_dataset_names = [\n",
                "    \"electricity_15min\",\n",
                "    \"monash_australian_electricity\",\n",
                "    \"monash_electricity_hourly\",\n",
                "    \"monash_electricity_weekly\",\n",
                "\n",
                "    \"monash_kdd_cup_2018\",\n",
                "\n",
                "    \"m4_daily\",\n",
                "    \"m4_hourly\",\n",
                "    \"m4_monthly\",\n",
                "    \"m4_weekly\",\n",
                "\n",
                "    \"mexico_city_bikes\",\n",
                "\n",
                "    \"solar\",\n",
                "    \"solar_1h\",\n",
                "\n",
                "    \"taxi_1h\",\n",
                "    \"taxi_30min\",\n",
                "\n",
                "    \"uber_tlc_daily\",\n",
                "    \"uber_tlc_hourly\",\n",
                "\n",
                "    \"ushcn_daily\",\n",
                "  \n",
                "    \"weatherbench_daily\",\n",
                "    \"weatherbench_hourly_temperature\",\n",
                "    \"weatherbench_weekly\",\n",
                "    \n",
                "    \"wiki_daily_100k\",\n",
                "\n",
                "    \"wind_farms_daily\",\n",
                "    \"wind_farms_hourly\",\n",
                "\n",
                "    \"monash_temperature_rain\",\n",
                "\n",
                "    \"monash_london_smart_meters\",\n",
                "\n",
                "]\n",
                "\n",
                "gifteval_dataset_names = [\n",
                "    \"alibaba_cluster_trace_2018\",\n",
                "    \"azure_vm_traces_2017\",\n",
                "    \"borg_cluster_data_2011\",\n",
                "    \"largest_2017\",\n",
                "    \"Q-TRAFFIC\",\n",
                "    \"buildings_900k\",\n",
                "]\n",
                "\n",
                "datasets_list = []\n",
                "datasets_list.append(HFDatasetAdapter(synthetic_train, name=\"synthetic_train\"))\n",
                "\n",
                "print(\"Loading real datasets...\")\n",
                "for name in chronos_dataset_names:\n",
                "    try:\n",
                "        ds = datasets.load_dataset(\"autogluon/chronos_datasets\", name, split=\"train\", streaming=True)\n",
                "        datasets_list.append(HFDatasetAdapter(ds))\n",
                "        print(f\"Loaded {name}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Could not load {name}: {e}\")\n",
                "\n",
                "for name in gifteval_dataset_names:\n",
                "    try:\n",
                "        ds = datasets.load_dataset(\"salesforce/gifteval_pretrain\", name, split=\"train\", streaming=True)\n",
                "        datasets_list.append(HFDatasetAdapter(ds))\n",
                "        print(f\"Loaded {name}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Could not load {name}: {e}\")\n",
                "\n",
                "# Calculate probabilities (Uniform for now, or weighted by size if known)\n",
                "# To fix the ValueError, we ensure len(probs) == len(datasets)\n",
                "num_datasets = len(datasets_list)\n",
                "probabilities = [1.0 / num_datasets] * num_datasets\n",
                "\n",
                "print(f\"Total datasets loaded: {num_datasets}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training Execution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create Streaming Datasets\n",
                "train_ds = StreamingChronosDataset(\n",
                "    datasets=datasets_list,\n",
                "    probabilities=probabilities,\n",
                "    context_length=CONTEXT_LENGTH,\n",
                "    prediction_length=PREDICTION_LENGTH,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    output_patch_size=PATCH_SIZE,\n",
                "    mode=DatasetMode.TRAIN,\n",
                ")\n",
                "\n",
                "# Validation (use synthetic only for speed/stability)\n",
                "val_ds = StreamingChronosDataset(\n",
                "    datasets=[HFDatasetAdapter(synthetic_val, name=\"synthetic_val\")],\n",
                "    probabilities=[1.0],\n",
                "    context_length=CONTEXT_LENGTH,\n",
                "    prediction_length=PREDICTION_LENGTH,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    output_patch_size=PATCH_SIZE,\n",
                "    mode=DatasetMode.VALIDATION,\n",
                ")\n",
                "\n",
                "# Training Arguments\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=CHECKPOINT_DIR,\n",
                "    per_device_train_batch_size=BATCH_SIZE,\n",
                "    per_device_eval_batch_size=BATCH_SIZE,\n",
                "    learning_rate=LEARNING_RATE,\n",
                "    lr_scheduler_type=\"linear\",\n",
                "    warmup_ratio=WARMUP_RATIO,\n",
                "    max_steps=MAX_STEPS,\n",
                "    save_steps=SAVE_STEPS,\n",
                "    logging_steps=LOGGING_STEPS,\n",
                "    save_strategy=\"steps\",\n",
                "    fp16=False,\n",
                "    dataloader_num_workers=0, # Must be 0 for IterableDataset with streaming usually, or handle carefully\n",
                "    dataloader_pin_memory=False, # Avoid warning if no GPU\n",
                "    remove_unused_columns=False,\n",
                "    report_to=\"wandb\",\n",
                "    run_name=RUN_NAME,\n",
                "    seed=SEED,\n",
                "    data_seed=SEED,\n",
                ")\n",
                "\n",
                "# Initialize Model (Same as before)\n",
                "chronos_forecasting_config = Chronos2ForecastingConfig(\n",
                "    context_length=CONTEXT_LENGTH,\n",
                "    output_patch_size=PATCH_SIZE,\n",
                "    input_patch_size=PATCH_SIZE,\n",
                "    input_patch_stride=PATCH_SIZE,\n",
                "    quantiles=QUANTILES,\n",
                "    time_encoding_scale=CONTEXT_LENGTH,\n",
                "    use_reg_token=True,\n",
                "    use_arcsinh=True,\n",
                "    max_output_patches=64,\n",
                ")\n",
                "\n",
                "model_config = Chronos2CoreConfig(\n",
                "    d_model=D_MODEL,\n",
                "    d_kv=D_KV,\n",
                "    d_ff=D_FF,\n",
                "    num_layers=NUM_LAYERS,\n",
                "    num_heads=NUM_HEADS,\n",
                "    dropout_rate=DROPOUT_RATE,\n",
                "    vocab_size=VOCAB_SIZE,\n",
                ")\n",
                "model_config.chronos_config = chronos_forecasting_config.__dict__\n",
                "model = Chronos2Model(model_config)\n",
                "\n",
                "# Trainer\n",
                "trainer = Chronos2Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_ds,\n",
                "    eval_dataset=val_ds,\n",
                ")\n",
                "\n",
                "print(\"Starting training...\")\n",
                "trainer.train()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
