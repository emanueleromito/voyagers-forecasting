{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Chronos-2 Training Baseline\n",
                "\n",
                "This notebook establishes a training baseline for Chronos-2 using synthetic data. It is designed to run on Google Colab."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone Repository\n",
                "!git clone https://github.com/emanueleromito/voyagers-forecasting.git\n",
                "%cd voyagers-forecasting\n",
                "\n",
                "# Create checkpoint directory\n",
                "import os\n",
                "CHECKPOINT_DIR = './checkpoints'\n",
                "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -e .[dev]\n",
                "!pip install gluonts transformers accelerate typer typer-config rich"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\n",
                "import math\n",
                "import random\n",
                "import sys\n",
                "import os\n",
                "from pathlib import Path\n",
                "from typing import List, Optional, Iterator\n",
                "from functools import partial\n",
                "import itertools\n",
                "\n",
                "# Ensure src is in python path\n",
                "sys.path.append(os.path.abspath(\"src\"))\n",
                "\n",
                "# Force reload modules if they are already loaded (crucial for Colab without restart)\n",
                "if 'legacy.chronos' in sys.modules:\n",
                "    del sys.modules['legacy.chronos']\n",
                "if 'legacy.chronos.chronos' in sys.modules:\n",
                "    del sys.modules['legacy.chronos.chronos']\n",
                "\n",
                "import numpy as np\n",
                "import torch\n",
                "from torch.utils.data import IterableDataset, get_worker_info\n",
                "import transformers\n",
                "from transformers import (\n",
                "    AutoConfig,\n",
                "    AutoModelForSeq2SeqLM,\n",
                "    Trainer,\n",
                "    TrainingArguments,\n",
                "    T5Config\n",
                ")\n",
                "from gluonts.dataset.common import FileDataset\n",
                "from gluonts.itertools import Cyclic, Map, Filter\n",
                "from gluonts.transform import (\n",
                "    FilterTransformation,\n",
                "    TestSplitSampler,\n",
                "    ValidationSplitSampler,\n",
                "    InstanceSplitter,\n",
                "    ExpectedNumInstanceSampler,\n",
                "    LeavesMissingValues,\n",
                ")\n",
                "\n",
                "# Import Chronos components\n",
                "from chronos2 import Chronos2Model, Chronos2Pipeline, Chronos2ForecastingConfig\n",
                "from legacy.chronos import ChronosConfig, ChronosTokenizer # Using legacy config for tokenizer compatibility if needed, or define new one\n",
                "\n",
                "# Setup logging\n",
                "logging.basicConfig(format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=logging.INFO)\n",
                "logger = logging.getLogger(__name__)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.1 Reproducibility\n",
                "\n",
                "We set random seeds to ensure reproducible results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def set_seed(seed: int = 42):\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    torch.cuda.manual_seed_all(seed)\n",
                "    transformers.set_seed(seed)\n",
                "    \n",
                "    # Ensure deterministic behavior in PyTorch (may impact performance)\n",
                "    torch.backends.cudnn.deterministic = True\n",
                "    torch.backends.cudnn.benchmark = False\n",
                "    \n",
                "    print(f\"Random seed set to {seed}\")\n",
                "\n",
                "SEED = 42\n",
                "set_seed(SEED)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Generation (Synthetic)\n",
                "\n",
                "We use a simplified version of KernelSynth to generate synthetic time series data on the fly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import functools\n",
                "from sklearn.gaussian_process import GaussianProcessRegressor\n",
                "from sklearn.gaussian_process.kernels import (\n",
                "    RBF,\n",
                "    ConstantKernel,\n",
                "    DotProduct,\n",
                "    ExpSineSquared,\n",
                "    Kernel,\n",
                "    RationalQuadratic,\n",
                "    WhiteKernel,\n",
                ")\n",
                "from gluonts.dataset.arrow import ArrowWriter\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# --- KernelSynth Logic ---\n",
                "LENGTH = 1024\n",
                "KERNEL_BANK = [\n",
                "    ExpSineSquared(periodicity=24 / LENGTH),\n",
                "    ExpSineSquared(periodicity=7 / LENGTH),\n",
                "    RBF(length_scale=0.1),\n",
                "    RationalQuadratic(alpha=0.1),\n",
                "    WhiteKernel(noise_level=0.1),\n",
                "    ConstantKernel(),\n",
                "]\n",
                "\n",
                "def random_binary_map(a: Kernel, b: Kernel):\n",
                "    binary_maps = [lambda x, y: x + y, lambda x, y: x * y]\n",
                "    return np.random.choice(binary_maps)(a, b)\n",
                "\n",
                "def sample_from_gp_prior(kernel: Kernel, X: np.ndarray, random_seed: Optional[int] = None):\n",
                "    if X.ndim == 1:\n",
                "        X = X[:, None]\n",
                "    gpr = GaussianProcessRegressor(kernel=kernel)\n",
                "    ts = gpr.sample_y(X, n_samples=1, random_state=random_seed)\n",
                "    return ts\n",
                "\n",
                "def generate_time_series(max_kernels: int = 3):\n",
                "    while True:\n",
                "        X = np.linspace(0, 1, LENGTH)\n",
                "        selected_kernels = np.random.choice(\n",
                "            KERNEL_BANK, np.random.randint(1, max_kernels + 1), replace=True\n",
                "        )\n",
                "        kernel = functools.reduce(random_binary_map, selected_kernels)\n",
                "        try:\n",
                "            ts = sample_from_gp_prior(kernel=kernel, X=X)\n",
                "            return {\"start\": np.datetime64(\"2000-01-01 00:00\", \"s\"), \"target\": ts.squeeze()}\n",
                "        except Exception:\n",
                "            continue\n",
                "\n",
                "# Generate Data\n",
                "DATA_PATH = Path(\"kernelsynth-data.arrow\")\n",
                "if not DATA_PATH.exists():\n",
                "    print(\"Generating synthetic data...\")\n",
                "    # Ensure we use the global seed for generation logic if relying on np.random\n",
                "    # The set_seed(SEED) call above handles np.random.seed\n",
                "    generated_dataset = [\n",
                "        generate_time_series(max_kernels=3)\n",
                "        for _ in tqdm(range(1000)) # Generate 1000 series for baseline\n",
                "    ]\n",
                "    ArrowWriter(compression=\"lz4\").write_to_file(\n",
                "        generated_dataset,\n",
                "        path=DATA_PATH,\n",
                "    )\n",
                "    print(f\"Data saved to {DATA_PATH}\")\n",
                "else:\n",
                "    print(f\"Data already exists at {DATA_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Dataset Class\n",
                "\n",
                "We define the `ChronosDataset` class to handle tokenization and formatting for the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PseudoShuffledIterableDataset(IterableDataset):\n",
                "    def __init__(self, base_dataset, shuffle_buffer_length: int = 100) -> None:\n",
                "        super().__init__()\n",
                "        self.base_dataset = base_dataset\n",
                "        self.shuffle_buffer_length = shuffle_buffer_length\n",
                "        self.generator = torch.Generator()\n",
                "        # Seed the generator for reproducibility\n",
                "        self.generator.manual_seed(SEED)\n",
                "\n",
                "    def __iter__(self):\n",
                "        shuffle_buffer = []\n",
                "        for element in self.base_dataset:\n",
                "            shuffle_buffer.append(element)\n",
                "            if len(shuffle_buffer) >= self.shuffle_buffer_length:\n",
                "                idx = torch.randint(len(shuffle_buffer), size=(), generator=self.generator)\n",
                "                yield shuffle_buffer.pop(idx)\n",
                "        while shuffle_buffer:\n",
                "            idx = torch.randint(len(shuffle_buffer), size=(), generator=self.generator)\n",
                "            yield shuffle_buffer.pop(idx)\n",
                "\n",
                "class ShuffleMixin:\n",
                "    def shuffle(self, shuffle_buffer_length: int = 100):\n",
                "        return PseudoShuffledIterableDataset(self, shuffle_buffer_length)\n",
                "\n",
                "class ChronosDataset(IterableDataset, ShuffleMixin):\n",
                "    def __init__(\n",
                "        self,\n",
                "        datasets: list,\n",
                "        probabilities: List[float],\n",
                "        tokenizer: ChronosTokenizer,\n",
                "        context_length: int = 512,\n",
                "        prediction_length: int = 64,\n",
                "        drop_prob: float = 0.2,\n",
                "        min_past: Optional[int] = None,\n",
                "        mode: str = \"training\",\n",
                "        np_dtype=np.float32,\n",
                "    ) -> None:\n",
                "        super().__init__()\n",
                "        self.datasets = datasets\n",
                "        self.probabilities = probabilities\n",
                "        self.tokenizer = tokenizer\n",
                "        self.context_length = context_length\n",
                "        self.prediction_length = prediction_length\n",
                "        self.drop_prob = drop_prob\n",
                "        self.min_past = min_past or prediction_length\n",
                "        self.mode = mode\n",
                "        self.np_dtype = np_dtype\n",
                "\n",
                "    def preprocess_entry(self, entry: dict, mode: str) -> dict:\n",
                "        entry = {f: entry[f] for f in [\"start\", \"target\"]}\n",
                "        entry[\"target\"] = np.asarray(entry[\"target\"], dtype=self.np_dtype)\n",
                "        if mode == \"training\" and self.drop_prob > 0:\n",
                "            target = entry[\"target\"].copy()\n",
                "            drop_p = np.random.uniform(low=0.0, high=self.drop_prob)\n",
                "            mask = np.random.choice([True, False], size=len(target), p=[drop_p, 1 - drop_p])\n",
                "            target[mask] = np.nan\n",
                "            entry[\"target\"] = target\n",
                "        return entry\n",
                "\n",
                "    def _create_instance_splitter(self, mode: str):\n",
                "        instance_sampler = {\n",
                "            \"training\": ExpectedNumInstanceSampler(num_instances=1.0, min_instances=1, min_past=self.min_past, min_future=self.prediction_length),\n",
                "            \"validation\": ValidationSplitSampler(min_future=self.prediction_length),\n",
                "        }[mode]\n",
                "        return InstanceSplitter(\n",
                "            target_field=\"target\", is_pad_field=\"is_pad\", start_field=\"start\", forecast_start_field=\"forecast_start\",\n",
                "            instance_sampler=instance_sampler, past_length=self.context_length, future_length=self.prediction_length, dummy_value=np.nan,\n",
                "        )\n",
                "\n",
                "    def create_training_data(self, data):\n",
                "        data = Cyclic(data)\n",
                "        split_transform = self._create_instance_splitter(\"training\") + FilterTransformation(condition=lambda entry: (~np.isnan(entry[\"past_target\"])).sum() > 0)\n",
                "        data = split_transform.apply(data, is_train=True)\n",
                "        return data\n",
                "\n",
                "    def create_validation_data(self, data):\n",
                "        data = self._create_instance_splitter(\"validation\").apply(data, is_train=False)\n",
                "        return data\n",
                "\n",
                "    def to_hf_format(self, entry: dict) -> dict:\n",
                "        past_target = torch.tensor(entry[\"past_target\"]).unsqueeze(0)\n",
                "        input_ids, attention_mask, scale = self.tokenizer.context_input_transform(past_target)\n",
                "        future_target = torch.tensor(entry[\"future_target\"]).unsqueeze(0)\n",
                "        labels, labels_mask = self.tokenizer.label_input_transform(future_target, scale)\n",
                "        labels[labels_mask == 0] = -100\n",
                "        return {\"input_ids\": input_ids.squeeze(0), \"attention_mask\": attention_mask.squeeze(0), \"labels\": labels.squeeze(0)}\n",
                "\n",
                "    def __iter__(self) -> Iterator:\n",
                "        preprocessed_datasets = [Map(partial(self.preprocess_entry, mode=self.mode), dataset) for dataset in self.datasets]\n",
                "        if self.mode == \"training\":\n",
                "            iterables = [self.create_training_data(dataset) for dataset in preprocessed_datasets]\n",
                "        else:\n",
                "            iterables = [self.create_validation_data(dataset) for dataset in preprocessed_datasets]\n",
                "        \n",
                "        iterators = list(map(iter, iterables))\n",
                "        if self.mode == \"training\":\n",
                "            while True:\n",
                "                idx = np.random.choice(range(len(iterators)), p=self.probabilities)\n",
                "                try:\n",
                "                    yield self.to_hf_format(next(iterators[idx]))\n",
                "                except StopIteration:\n",
                "                    return\n",
                "        else:\n",
                "            for entry in itertools.chain(*iterators):\n",
                "                yield self.to_hf_format(entry)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Initialization & Training\n",
                "\n",
                "We initialize Chronos-2 using `google/t5-efficient-tiny` config as requested."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "CONTEXT_LENGTH = 512\n",
                "PREDICTION_LENGTH = 64\n",
                "MODEL_ID = \"google/t5-efficient-tiny\" # Requested by user\n",
                "\n",
                "# Load Model Config\n",
                "config = AutoConfig.from_pretrained(MODEL_ID)\n",
                "config.initializer_factor = 0.05 # Recommended for T5\n",
                "\n",
                "# Initialize Model\n",
                "model = AutoModelForSeq2SeqLM.from_config(config)\n",
                "print(f\"Model Parameters: {model.num_parameters() / 1e6:.2f}M\")\n",
                "\n",
                "# Chronos Config (for Tokenizer)\n",
                "chronos_config = ChronosConfig(\n",
                "    tokenizer_class=\"MeanScaleUniformBins\",\n",
                "    tokenizer_kwargs={'low_limit': -15.0, 'high_limit': 15.0},\n",
                "    n_tokens=4096,\n",
                "    n_special_tokens=2,\n",
                "    pad_token_id=0,\n",
                "    eos_token_id=1,\n",
                "    use_eos_token=True,\n",
                "    model_type=\"seq2seq\",\n",
                "    context_length=CONTEXT_LENGTH,\n",
                "    prediction_length=PREDICTION_LENGTH,\n",
                "    num_samples=20,\n",
                "    temperature=1.0,\n",
                "    top_k=50,\n",
                "    top_p=1.0,\n",
                ")\n",
                "model.config.chronos_config = chronos_config.__dict__\n",
                "\n",
                "# Resize embeddings to match tokenizer\n",
                "model.resize_token_embeddings(chronos_config.n_tokens)\n",
                "\n",
                "# Prepare Dataset\n",
                "train_ds = ChronosDataset(\n",
                "    datasets=[FileDataset(DATA_PATH, freq=\"h\")],\n",
                "    probabilities=[1.0],\n",
                "    tokenizer=chronos_config.create_tokenizer(),\n",
                "    context_length=CONTEXT_LENGTH,\n",
                "    prediction_length=PREDICTION_LENGTH,\n",
                "    mode=\"training\"\n",
                ").shuffle(shuffle_buffer_length=1000)\n",
                "\n",
                "# Training Arguments\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=CHECKPOINT_DIR,\n",
                "    per_device_train_batch_size=32,\n",
                "    learning_rate=1e-3,\n",
                "    num_train_epochs=5,\n",
                "    logging_steps=10,\n",
                "    save_strategy=\"epoch\",\n",
                "    fp16=True, # Mixed Precision\n",
                "    dataloader_num_workers=2,\n",
                "    remove_unused_columns=False,\n",
                "    report_to=\"none\",\n",
                "    seed=SEED, # Set seed for Trainer\n",
                "    data_seed=SEED, # Set seed for data loading\n",
                ")\n",
                "\n",
                "# Trainer\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_ds,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Run Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sanity Check: Train for a few steps\n",
                "print(\"Starting training...\")\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Validation\n",
                "\n",
                "We perform a quick validation to check the loss."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "val_ds = ChronosDataset(\n",
                "    datasets=[FileDataset(DATA_PATH, freq=\"h\")],\n",
                "    probabilities=[1.0],\n",
                "    tokenizer=chronos_config.create_tokenizer(),\n",
                "    context_length=CONTEXT_LENGTH,\n",
                "    prediction_length=PREDICTION_LENGTH,\n",
                "    mode=\"validation\"\n",
                ")\n",
                "\n",
                "# Evaluate\n",
                "eval_results = trainer.evaluate(val_ds)\n",
                "print(f\"Validation Loss: {eval_results['eval_loss']}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}